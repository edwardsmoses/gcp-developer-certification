{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef08f1e9-2af5-4b3b-be45-7f9123dae90c",
   "metadata": {},
   "source": [
    "# Babyweight Data Preprocessing with tf.Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a716bc-ce8d-4f29-a9d2-c83371137b81",
   "metadata": {},
   "source": [
    "## 1. Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80870765-75f7-4bce-a49f-d6dcdc76b066",
   "metadata": {},
   "source": [
    "### Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adef56d2-e756-441a-b7f2-907a67b51b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting apache-beam==2.36.0\n",
      "  Downloading apache_beam-2.36.0-cp37-cp37m-manylinux2010_x86_64.whl (10.1 MB)\n",
      "     |████████████████████████████████| 10.1 MB 10.7 MB/s            \n",
      "\u001b[?25hCollecting pyparsing==2.4.2\n",
      "  Downloading pyparsing-2.4.2-py2.py3-none-any.whl (65 kB)\n",
      "     |████████████████████████████████| 65 kB 3.0 MB/s             \n",
      "\u001b[?25hRequirement already satisfied: orjson<4.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam==2.36.0) (3.6.5)\n",
      "Requirement already satisfied: crcmod<2.0,>=1.7 in /opt/conda/lib/python3.7/site-packages (from apache-beam==2.36.0) (1.7)\n",
      "Requirement already satisfied: cloudpickle<3,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam==2.36.0) (2.0.0)\n",
      "Requirement already satisfied: pyarrow<7.0.0,>=0.15.1 in /opt/conda/lib/python3.7/site-packages (from apache-beam==2.36.0) (6.0.1)\n",
      "Requirement already satisfied: pytz>=2018.3 in /opt/conda/lib/python3.7/site-packages (from apache-beam==2.36.0) (2021.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.24.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam==2.36.0) (2.26.0)\n",
      "Collecting httplib2<0.20.0,>=0.8\n",
      "  Downloading httplib2-0.19.1-py3-none-any.whl (95 kB)\n",
      "     |████████████████████████████████| 95 kB 3.9 MB/s             \n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam==2.36.0) (4.0.1)\n",
      "Collecting dill<0.3.2,>=0.3.1.1\n",
      "  Downloading dill-0.3.1.1.tar.gz (151 kB)\n",
      "     |████████████████████████████████| 151 kB 58.9 MB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil<3,>=2.8.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam==2.36.0) (2.8.2)\n",
      "Requirement already satisfied: pydot<2,>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam==2.36.0) (1.4.2)\n",
      "Requirement already satisfied: proto-plus<2,>=1.7.1 in /opt/conda/lib/python3.7/site-packages (from apache-beam==2.36.0) (1.19.8)\n",
      "Requirement already satisfied: fastavro<2,>=0.21.4 in /opt/conda/lib/python3.7/site-packages (from apache-beam==2.36.0) (1.4.7)\n",
      "Requirement already satisfied: pymongo<4.0.0,>=3.8.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam==2.36.0) (3.12.3)\n",
      "Requirement already satisfied: hdfs<3.0.0,>=2.1.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam==2.36.0) (2.6.0)\n",
      "Requirement already satisfied: protobuf<4,>=3.12.2 in /opt/conda/lib/python3.7/site-packages (from apache-beam==2.36.0) (3.19.1)\n",
      "Requirement already satisfied: grpcio<2,>=1.29.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam==2.36.0) (1.43.0)\n",
      "Requirement already satisfied: numpy<1.22.0,>=1.14.3 in /opt/conda/lib/python3.7/site-packages (from apache-beam==2.36.0) (1.19.5)\n",
      "Requirement already satisfied: oauth2client<5,>=2.0.1 in /opt/conda/lib/python3.7/site-packages (from apache-beam==2.36.0) (4.1.3)\n",
      "Requirement already satisfied: six>=1.5.2 in /opt/conda/lib/python3.7/site-packages (from grpcio<2,>=1.29.0->apache-beam==2.36.0) (1.16.0)\n",
      "Requirement already satisfied: docopt in /opt/conda/lib/python3.7/site-packages (from hdfs<3.0.0,>=2.1.0->apache-beam==2.36.0) (0.6.2)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in /opt/conda/lib/python3.7/site-packages (from oauth2client<5,>=2.0.1->apache-beam==2.36.0) (0.4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.0.5 in /opt/conda/lib/python3.7/site-packages (from oauth2client<5,>=2.0.1->apache-beam==2.36.0) (0.2.7)\n",
      "Requirement already satisfied: rsa>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from oauth2client<5,>=2.0.1->apache-beam==2.36.0) (4.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.24.0->apache-beam==2.36.0) (3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.24.0->apache-beam==2.36.0) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.24.0->apache-beam==2.36.0) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.24.0->apache-beam==2.36.0) (2.0.9)\n",
      "Building wheels for collected packages: dill\n",
      "  Building wheel for dill (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78546 sha256=8182d83b855fd51217fad1d3353ae8a48a57ee396f732fe5460ec62a4d17dac2\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/a4/61/fd/c57e374e580aa78a45ed78d5859b3a44436af17e22ca53284f\n",
      "Successfully built dill\n",
      "Installing collected packages: pyparsing, httplib2, dill, apache-beam\n",
      "  Attempting uninstall: pyparsing\n",
      "    Found existing installation: pyparsing 3.0.6\n",
      "    Uninstalling pyparsing-3.0.6:\n",
      "      Successfully uninstalled pyparsing-3.0.6\n",
      "  Attempting uninstall: httplib2\n",
      "    Found existing installation: httplib2 0.20.2\n",
      "    Uninstalling httplib2-0.20.2:\n",
      "      Successfully uninstalled httplib2-0.20.2\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.4\n",
      "    Uninstalling dill-0.3.4:\n",
      "      Successfully uninstalled dill-0.3.4\n",
      "  Attempting uninstall: apache-beam\n",
      "    Found existing installation: apache-beam 2.34.0\n",
      "    Uninstalling apache-beam-2.34.0:\n",
      "      Successfully uninstalled apache-beam-2.34.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "explainable-ai-sdk 1.3.2 requires xai-image-widget, which is not installed.\n",
      "tfx-bsl 1.5.0 requires absl-py<0.13,>=0.9, but you have absl-py 1.0.0 which is incompatible.\n",
      "tfx-bsl 1.5.0 requires google-api-python-client<2,>=1.7.11, but you have google-api-python-client 2.33.0 which is incompatible.\n",
      "tfx-bsl 1.5.0 requires pyarrow<6,>=1, but you have pyarrow 6.0.1 which is incompatible.\n",
      "tensorflow-transform 1.5.0 requires absl-py<0.13,>=0.9, but you have absl-py 1.0.0 which is incompatible.\n",
      "tensorflow-transform 1.5.0 requires pyarrow<6,>=1, but you have pyarrow 6.0.1 which is incompatible.\n",
      "cloud-tpu-client 0.10 requires google-api-python-client==1.8.0, but you have google-api-python-client 2.33.0 which is incompatible.\u001b[0m\n",
      "Successfully installed apache-beam-2.36.0 dill-0.3.1.1 httplib2-0.19.1 pyparsing-2.4.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install apache-beam==2.36.0 pyparsing==2.4.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbdad52-9615-4153-81f7-d1fff38b6cd0",
   "metadata": {},
   "source": [
    "You can ingnore the dependency resolver errors. Confirm the final message starting with \"Successfully installed ...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "206794d5-3682-47e2-bd83-de167401be7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-transform==1.6.0\n",
      "  Downloading tensorflow_transform-1.6.0-py3-none-any.whl (427 kB)\n",
      "     |████████████████████████████████| 427 kB 9.3 MB/s            \n",
      "\u001b[?25hCollecting apache-beam[gcp]<3,>=2.35\n",
      "  Using cached apache_beam-2.36.0-cp37-cp37m-manylinux2010_x86_64.whl (10.1 MB)\n",
      "Collecting tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,<2.8,>=1.15.5\n",
      "  Downloading tensorflow-2.7.1-cp37-cp37m-manylinux2010_x86_64.whl (495.0 MB)\n",
      "     |████████████████████████████████| 495.0 MB 19 kB/s              05               | 131.1 MB 76.6 MB/s eta 0:00:05B 75.3 MB/s eta 0:00:02\n",
      "\u001b[?25hCollecting pydot<2,>=1.2\n",
      "  Downloading pydot-1.4.2-py2.py3-none-any.whl (21 kB)\n",
      "Collecting pyarrow<6,>=1\n",
      "  Downloading pyarrow-5.0.0-cp37-cp37m-manylinux2014_x86_64.whl (23.6 MB)\n",
      "     |████████████████████████████████| 23.6 MB 42.3 MB/s            \n",
      "\u001b[?25hCollecting tfx-bsl<1.7.0,>=1.6.0\n",
      "  Downloading tfx_bsl-1.6.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (19.1 MB)\n",
      "     |████████████████████████████████| 19.1 MB 35.5 MB/s            \n",
      "\u001b[?25hCollecting tensorflow-metadata<1.7.0,>=1.6.0\n",
      "  Downloading tensorflow_metadata-1.6.0-py3-none-any.whl (48 kB)\n",
      "     |████████████████████████████████| 48 kB 9.3 MB/s             \n",
      "\u001b[?25hCollecting protobuf<4,>=3.13\n",
      "  Downloading protobuf-3.19.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "     |████████████████████████████████| 1.1 MB 61.8 MB/s            \n",
      "\u001b[?25hCollecting absl-py<2.0.0,>=0.9\n",
      "  Downloading absl_py-1.0.0-py3-none-any.whl (126 kB)\n",
      "     |████████████████████████████████| 126 kB 55.3 MB/s            \n",
      "\u001b[?25hCollecting numpy<2,>=1.16\n",
      "  Downloading numpy-1.21.5-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
      "     |████████████████████████████████| 15.7 MB 43.5 MB/s            \n",
      "\u001b[?25hCollecting six\n",
      "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting orjson<4.0\n",
      "  Downloading orjson-3.6.7-cp37-cp37m-manylinux_2_24_x86_64.whl (255 kB)\n",
      "     |████████████████████████████████| 255 kB 62.3 MB/s            \n",
      "\u001b[?25hCollecting crcmod<2.0,>=1.7\n",
      "  Downloading crcmod-1.7.tar.gz (89 kB)\n",
      "     |████████████████████████████████| 89 kB 228 kB/s             \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting fastavro<2,>=0.21.4\n",
      "  Downloading fastavro-1.4.9-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
      "     |████████████████████████████████| 2.3 MB 30.7 MB/s            \n",
      "\u001b[?25hCollecting proto-plus<2,>=1.7.1\n",
      "  Downloading proto_plus-1.20.3-py3-none-any.whl (46 kB)\n",
      "     |████████████████████████████████| 46 kB 4.7 MB/s             \n",
      "\u001b[?25hCollecting cloudpickle<3,>=2.0.0\n",
      "  Downloading cloudpickle-2.0.0-py3-none-any.whl (25 kB)\n",
      "Collecting python-dateutil<3,>=2.8.0\n",
      "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "     |████████████████████████████████| 247 kB 59.5 MB/s            \n",
      "\u001b[?25hCollecting oauth2client<5,>=2.0.1\n",
      "  Downloading oauth2client-4.1.3-py2.py3-none-any.whl (98 kB)\n",
      "     |████████████████████████████████| 98 kB 9.1 MB/s             \n",
      "\u001b[?25hCollecting grpcio<2,>=1.29.0\n",
      "  Downloading grpcio-1.44.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n",
      "     |████████████████████████████████| 4.3 MB 65.0 MB/s            \n",
      "\u001b[?25hCollecting typing-extensions>=3.7.0\n",
      "  Downloading typing_extensions-4.1.1-py3-none-any.whl (26 kB)\n",
      "Collecting pymongo<4.0.0,>=3.8.0\n",
      "  Downloading pymongo-3.12.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (508 kB)\n",
      "     |████████████████████████████████| 508 kB 52.2 MB/s            \n",
      "\u001b[?25hCollecting dill<0.3.2,>=0.3.1.1\n",
      "  Using cached dill-0.3.1.1-py3-none-any.whl\n",
      "Collecting requests<3.0.0,>=2.24.0\n",
      "  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
      "     |████████████████████████████████| 63 kB 2.6 MB/s             \n",
      "\u001b[?25hCollecting httplib2<0.20.0,>=0.8\n",
      "  Using cached httplib2-0.19.1-py3-none-any.whl (95 kB)\n",
      "Collecting hdfs<3.0.0,>=2.1.0\n",
      "  Downloading hdfs-2.6.0-py3-none-any.whl (33 kB)\n",
      "Collecting pytz>=2018.3\n",
      "  Downloading pytz-2021.3-py2.py3-none-any.whl (503 kB)\n",
      "     |████████████████████████████████| 503 kB 45.7 MB/s            \n",
      "\u001b[?25hCollecting google-cloud-recommendations-ai<=0.2.0,>=0.1.0\n",
      "  Downloading google_cloud_recommendations_ai-0.2.0-py2.py3-none-any.whl (180 kB)\n",
      "     |████████████████████████████████| 180 kB 70.9 MB/s            \n",
      "\u001b[?25hCollecting google-cloud-vision<2,>=0.38.0\n",
      "  Downloading google_cloud_vision-1.0.0-py2.py3-none-any.whl (435 kB)\n",
      "     |████████████████████████████████| 435 kB 51.5 MB/s            \n",
      "\u001b[?25hCollecting google-apitools<0.5.32,>=0.5.31\n",
      "  Downloading google-apitools-0.5.31.tar.gz (173 kB)\n",
      "     |████████████████████████████████| 173 kB 24.5 MB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting google-cloud-pubsub<3,>=2.1.0\n",
      "  Downloading google_cloud_pubsub-2.9.0-py2.py3-none-any.whl (219 kB)\n",
      "     |████████████████████████████████| 219 kB 53.0 MB/s            \n",
      "\u001b[?25hCollecting google-cloud-pubsublite<2,>=1.2.0\n",
      "  Downloading google_cloud_pubsublite-1.4.0-py2.py3-none-any.whl (261 kB)\n",
      "     |████████████████████████████████| 261 kB 32.1 MB/s            \n",
      "\u001b[?25hCollecting google-cloud-dlp<4,>=3.0.0\n",
      "  Downloading google_cloud_dlp-3.6.1-py2.py3-none-any.whl (113 kB)\n",
      "     |████████████████████████████████| 113 kB 54.9 MB/s            \n",
      "\u001b[?25hCollecting cachetools<5,>=3.1.0\n",
      "  Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\n",
      "Collecting google-cloud-language<2,>=1.3.0\n",
      "  Downloading google_cloud_language-1.3.0-py2.py3-none-any.whl (83 kB)\n",
      "     |████████████████████████████████| 83 kB 3.0 MB/s             \n",
      "\u001b[?25hCollecting google-auth<3,>=1.18.0\n",
      "  Downloading google_auth-2.6.0-py2.py3-none-any.whl (156 kB)\n",
      "     |████████████████████████████████| 156 kB 58.9 MB/s            \n",
      "\u001b[?25hCollecting google-cloud-bigtable<2,>=0.31.1\n",
      "  Downloading google_cloud_bigtable-1.7.0-py2.py3-none-any.whl (267 kB)\n",
      "     |████████████████████████████████| 267 kB 57.4 MB/s            \n",
      "\u001b[?25hCollecting grpcio-gcp<1,>=0.2.2\n",
      "  Downloading grpcio_gcp-0.2.2-py2.py3-none-any.whl (9.4 kB)\n",
      "Collecting google-cloud-videointelligence<2,>=1.8.0\n",
      "  Downloading google_cloud_videointelligence-1.16.1-py2.py3-none-any.whl (183 kB)\n",
      "     |████████████████████████████████| 183 kB 50.6 MB/s            \n",
      "\u001b[?25hCollecting google-cloud-bigquery-storage>=2.6.3\n",
      "  Downloading google_cloud_bigquery_storage-2.12.0-py2.py3-none-any.whl (179 kB)\n",
      "     |████████████████████████████████| 179 kB 9.6 MB/s            \n",
      "\u001b[?25hCollecting google-cloud-spanner<2,>=1.13.0\n",
      "  Downloading google_cloud_spanner-1.19.1-py2.py3-none-any.whl (255 kB)\n",
      "     |████████████████████████████████| 255 kB 62.2 MB/s            \n",
      "\u001b[?25hCollecting google-cloud-datastore<2,>=1.8.0\n",
      "  Downloading google_cloud_datastore-1.15.3-py2.py3-none-any.whl (134 kB)\n",
      "     |████████████████████████████████| 134 kB 52.9 MB/s            \n",
      "\u001b[?25hCollecting google-cloud-bigquery<3,>=1.6.0\n",
      "  Downloading google_cloud_bigquery-2.34.1-py2.py3-none-any.whl (206 kB)\n",
      "     |████████████████████████████████| 206 kB 45.1 MB/s            \n",
      "\u001b[?25hCollecting google-cloud-core<2,>=0.28.1\n",
      "  Downloading google_cloud_core-1.7.2-py2.py3-none-any.whl (28 kB)\n",
      "Collecting pyparsing>=2.1.4\n",
      "  Downloading pyparsing-3.0.7-py3-none-any.whl (98 kB)\n",
      "     |████████████████████████████████| 98 kB 10.0 MB/s            \n",
      "\u001b[?25hCollecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "     |████████████████████████████████| 65 kB 1.8 MB/s             \n",
      "\u001b[?25hCollecting wheel<1.0,>=0.32.0\n",
      "  Downloading wheel-0.37.1-py2.py3-none-any.whl (35 kB)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting keras-preprocessing>=1.1.1\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "     |████████████████████████████████| 42 kB 2.2 MB/s             \n",
      "\u001b[?25hCollecting tensorflow-io-gcs-filesystem>=0.21.0\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.24.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.1 MB)\n",
      "     |████████████████████████████████| 2.1 MB 46.1 MB/s            \n",
      "\u001b[?25hCollecting flatbuffers<3.0,>=1.12\n",
      "  Downloading flatbuffers-2.0-py2.py3-none-any.whl (26 kB)\n",
      "Collecting tensorboard~=2.6\n",
      "  Downloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
      "     |████████████████████████████████| 5.8 MB 18.7 MB/s            \n",
      "\u001b[?25hCollecting libclang>=9.0.1\n",
      "  Downloading libclang-13.0.0-py2.py3-none-manylinux1_x86_64.whl (14.5 MB)\n",
      "     |████████████████████████████████| 14.5 MB 39.3 MB/s            \n",
      "\u001b[?25hCollecting h5py>=2.9.0\n",
      "  Downloading h5py-3.6.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.1 MB)\n",
      "     |████████████████████████████████| 4.1 MB 53.8 MB/s            \n",
      "\u001b[?25hCollecting wrapt>=1.11.0\n",
      "  Downloading wrapt-1.13.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (79 kB)\n",
      "     |████████████████████████████████| 79 kB 11.7 MB/s            \n",
      "\u001b[?25hCollecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "     |████████████████████████████████| 57 kB 9.2 MB/s             \n",
      "\u001b[?25hCollecting keras<2.8,>=2.7.0rc0\n",
      "  Downloading keras-2.7.0-py2.py3-none-any.whl (1.3 MB)\n",
      "     |████████████████████████████████| 1.3 MB 52.3 MB/s            \n",
      "\u001b[?25hCollecting termcolor>=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting tensorflow-estimator<2.8,~=2.7.0rc0\n",
      "  Downloading tensorflow_estimator-2.7.0-py2.py3-none-any.whl (463 kB)\n",
      "     |████████████████████████████████| 463 kB 49.0 MB/s            \n",
      "\u001b[?25hCollecting gast<0.5.0,>=0.2.1\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting googleapis-common-protos<2,>=1.52.0\n",
      "  Downloading googleapis_common_protos-1.55.0-py2.py3-none-any.whl (212 kB)\n",
      "     |████████████████████████████████| 212 kB 53.7 MB/s            \n",
      "\u001b[?25hCollecting tensorflow-serving-api!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,<3,>=1.15\n",
      "  Downloading tensorflow_serving_api-2.8.0-py2.py3-none-any.whl (37 kB)\n",
      "Collecting google-api-python-client<2,>=1.7.11\n",
      "  Downloading google_api_python_client-1.12.10-py2.py3-none-any.whl (61 kB)\n",
      "     |████████████████████████████████| 61 kB 182 kB/s             \n",
      "\u001b[?25hCollecting pandas<2,>=1.0\n",
      "  Downloading pandas-1.3.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
      "     |████████████████████████████████| 11.3 MB 64.7 MB/s            \n",
      "\u001b[?25hCollecting google-auth-httplib2>=0.0.3\n",
      "  Downloading google_auth_httplib2-0.1.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Collecting google-api-core<3dev,>=1.21.0\n",
      "  Downloading google_api_core-2.6.0-py2.py3-none-any.whl (114 kB)\n",
      "     |████████████████████████████████| 114 kB 67.4 MB/s            \n",
      "\u001b[?25hCollecting uritemplate<4dev,>=3.0.0\n",
      "  Downloading uritemplate-3.0.1-py2.py3-none-any.whl (15 kB)\n",
      "Collecting fasteners>=0.14\n",
      "  Downloading fasteners-0.17.3-py3-none-any.whl (18 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.8-py3-none-any.whl (39 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "     |████████████████████████████████| 155 kB 60.6 MB/s            \n",
      "\u001b[?25hCollecting google-resumable-media<3.0dev,>=0.6.0\n",
      "  Downloading google_resumable_media-2.3.1-py2.py3-none-any.whl (76 kB)\n",
      "     |████████████████████████████████| 76 kB 8.4 MB/s             \n",
      "\u001b[?25hCollecting packaging>=14.3\n",
      "  Downloading packaging-21.3-py3-none-any.whl (40 kB)\n",
      "     |████████████████████████████████| 40 kB 8.3 MB/s             \n",
      "\u001b[?25hCollecting google-api-core[grpc]<3.0.0dev,>=1.29.0\n",
      "  Downloading google_api_core-1.31.5-py2.py3-none-any.whl (93 kB)\n",
      "     |████████████████████████████████| 93 kB 2.7 MB/s             \n",
      "\u001b[?25hCollecting grpc-google-iam-v1<0.13dev,>=0.12.3\n",
      "  Downloading grpc-google-iam-v1-0.12.3.tar.gz (13 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting google-auth<3,>=1.18.0\n",
      "  Downloading google_auth-1.35.0-py2.py3-none-any.whl (152 kB)\n",
      "     |████████████████████████████████| 152 kB 53.3 MB/s            \n",
      "\u001b[?25hCollecting setuptools>=40.3.0\n",
      "  Downloading setuptools-60.9.3-py3-none-any.whl (1.1 MB)\n",
      "     |████████████████████████████████| 1.1 MB 53.0 MB/s            \n",
      "\u001b[?25hCollecting libcst>=0.3.10\n",
      "  Downloading libcst-0.4.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
      "     |████████████████████████████████| 2.7 MB 30.2 MB/s            \n",
      "\u001b[?25hCollecting grpcio-status>=1.18.0\n",
      "  Downloading grpcio_status-1.44.0-py3-none-any.whl (10.0 kB)\n",
      "Collecting overrides<7.0.0,>=6.0.1\n",
      "  Downloading overrides-6.1.0-py3-none-any.whl (14 kB)\n",
      "Collecting cached-property\n",
      "  Downloading cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Collecting docopt\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pyparsing>=2.1.4\n",
      "  Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
      "     |████████████████████████████████| 67 kB 9.3 MB/s             \n",
      "\u001b[?25hCollecting pyasn1>=0.1.7\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "     |████████████████████████████████| 77 kB 8.5 MB/s             \n",
      "\u001b[?25hCollecting idna<4,>=2.5\n",
      "  Downloading idna-3.3-py3-none-any.whl (61 kB)\n",
      "     |████████████████████████████████| 61 kB 12.4 MB/s            \n",
      "\u001b[?25hCollecting certifi>=2017.4.17\n",
      "  Downloading certifi-2021.10.8-py2.py3-none-any.whl (149 kB)\n",
      "     |████████████████████████████████| 149 kB 50.2 MB/s            \n",
      "\u001b[?25hCollecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.8-py2.py3-none-any.whl (138 kB)\n",
      "     |████████████████████████████████| 138 kB 66.9 MB/s            \n",
      "\u001b[?25hCollecting charset-normalizer~=2.0.0\n",
      "  Downloading charset_normalizer-2.0.12-py3-none-any.whl (39 kB)\n",
      "Collecting werkzeug>=0.11.15\n",
      "  Downloading Werkzeug-2.0.3-py3-none-any.whl (289 kB)\n",
      "     |████████████████████████████████| 289 kB 60.1 MB/s            \n",
      "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.6-py3-none-any.whl (97 kB)\n",
      "     |████████████████████████████████| 97 kB 9.5 MB/s             \n",
      "\u001b[?25hCollecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "     |████████████████████████████████| 4.9 MB 24.5 MB/s            \n",
      "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "     |████████████████████████████████| 781 kB 36.4 MB/s            \n",
      "\u001b[?25hCollecting tensorflow-serving-api!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,<3,>=1.15\n",
      "  Downloading tensorflow_serving_api-2.7.0-py2.py3-none-any.whl (37 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting google-crc32c<2.0dev,>=1.0\n",
      "  Downloading google_crc32c-1.3.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38 kB)\n",
      "Collecting typing-inspect>=0.4.0\n",
      "  Downloading typing_inspect-0.7.1-py3-none-any.whl (8.4 kB)\n",
      "Collecting pyyaml>=5.2\n",
      "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
      "     |████████████████████████████████| 596 kB 59.7 MB/s            \n",
      "\u001b[?25hCollecting importlib-metadata>=4.4\n",
      "  Downloading importlib_metadata-4.11.2-py3-none-any.whl (17 kB)\n",
      "Collecting typing-utils>=0.0.3\n",
      "  Downloading typing_utils-0.1.0-py3-none-any.whl (10 kB)\n",
      "Collecting zipp>=0.5\n",
      "  Downloading zipp-3.7.0-py3-none-any.whl (5.3 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
      "     |████████████████████████████████| 151 kB 59.8 MB/s            \n",
      "\u001b[?25hCollecting mypy-extensions>=0.3.0\n",
      "  Downloading mypy_extensions-0.4.3-py2.py3-none-any.whl (4.5 kB)\n",
      "Building wheels for collected packages: crcmod, google-apitools, termcolor, grpc-google-iam-v1, docopt\n",
      "  Building wheel for crcmod (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for crcmod: filename=crcmod-1.7-cp37-cp37m-linux_x86_64.whl size=36825 sha256=984ed292bab41b3cb7c58c92ef68acbed17b3f7ab91d191ce8a5d21c8a440a53\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/dc/9a/e9/49e627353476cec8484343c4ab656f1e0d783ee77b9dde2d1f\n",
      "  Building wheel for google-apitools (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for google-apitools: filename=google_apitools-0.5.31-py3-none-any.whl size=131040 sha256=638507a8903f81bf7b59c0bf6a7ba8d4a61eb8d6559eb2ba16998589ee5e1039\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/19/b5/2f/1cc3cf2b31e7a9cd1508731212526d9550271274d351c96f16\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4847 sha256=56dddd4400f5bfbee474de830a168ea8fc9c4c04dff31f5d6a083d3c8a48cde6\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
      "  Building wheel for grpc-google-iam-v1 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for grpc-google-iam-v1: filename=grpc_google_iam_v1-0.12.3-py3-none-any.whl size=18515 sha256=9b292eb29fb11e5e8112f05e9a7242a93ae0aadc0d3abf347c38212d5100ff2e\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/b9/ee/67/2e444183030cb8d31ce8b34cee34a7afdbd3ba5959ea846380\n",
      "  Building wheel for docopt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13724 sha256=fbbf84c9db2faae2a952b73f97cb4f2a3f1d3f54ba6711774419261bcd863d87\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/72/b0/3f/1d95f96ff986c7dfffe46ce2be4062f38ebd04b506c77c81b9\n",
      "Successfully built crcmod google-apitools termcolor grpc-google-iam-v1 docopt\n",
      "Installing collected packages: pyasn1, urllib3, six, setuptools, rsa, pyparsing, pyasn1-modules, protobuf, idna, charset-normalizer, certifi, cachetools, zipp, typing-extensions, requests, pytz, packaging, oauthlib, mypy-extensions, grpcio, googleapis-common-protos, google-auth, typing-inspect, requests-oauthlib, pyyaml, importlib-metadata, google-api-core, wheel, werkzeug, typing-utils, tensorboard-plugin-wit, tensorboard-data-server, proto-plus, numpy, markdown, libcst, httplib2, grpcio-gcp, grpc-google-iam-v1, google-crc32c, google-auth-oauthlib, docopt, cached-property, absl-py, wrapt, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard, python-dateutil, pymongo, pydot, pyarrow, overrides, orjson, opt-einsum, oauth2client, libclang, keras-preprocessing, keras, hdfs, h5py, grpcio-status, google-resumable-media, google-pasta, google-cloud-pubsub, google-cloud-core, gast, flatbuffers, fasteners, fastavro, dill, crcmod, cloudpickle, astunparse, uritemplate, tensorflow, google-cloud-vision, google-cloud-videointelligence, google-cloud-spanner, google-cloud-recommendations-ai, google-cloud-pubsublite, google-cloud-language, google-cloud-dlp, google-cloud-datastore, google-cloud-bigtable, google-cloud-bigquery-storage, google-cloud-bigquery, google-auth-httplib2, google-apitools, apache-beam, tensorflow-serving-api, tensorflow-metadata, pandas, google-api-python-client, tfx-bsl, tensorflow-transform\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "explainable-ai-sdk 1.3.2 requires xai-image-widget, which is not installed.\n",
      "tensorflow-io 0.21.0 requires tensorflow<2.7.0,>=2.6.0, but you have tensorflow 2.7.1 which is incompatible.\n",
      "tensorflow-io 0.21.0 requires tensorflow-io-gcs-filesystem==0.21.0, but you have tensorflow-io-gcs-filesystem 0.24.0 which is incompatible.\n",
      "pandas-profiling 3.0.0 requires tangled-up-in-unicode==0.1.0, but you have tangled-up-in-unicode 0.2.0 which is incompatible.\n",
      "numba 0.54.1 requires numpy<1.21,>=1.17, but you have numpy 1.21.5 which is incompatible.\n",
      "cloud-tpu-client 0.10 requires google-api-python-client==1.8.0, but you have google-api-python-client 1.12.10 which is incompatible.\u001b[0m\n",
      "Successfully installed absl-py-1.0.0 apache-beam-2.36.0 astunparse-1.6.3 cached-property-1.5.2 cachetools-4.2.4 certifi-2021.10.8 charset-normalizer-2.0.12 cloudpickle-2.0.0 crcmod-1.7 dill-0.3.1.1 docopt-0.6.2 fastavro-1.4.9 fasteners-0.17.3 flatbuffers-2.0 gast-0.4.0 google-api-core-2.3.2 google-api-python-client-2.33.0 google-apitools-0.5.31 google-auth-2.3.3 google-auth-httplib2-0.1.0 google-auth-oauthlib-0.4.6 google-cloud-bigquery-2.34.1 google-cloud-bigquery-storage-2.12.0 google-cloud-bigtable-2.4.0 google-cloud-core-2.2.1 google-cloud-datastore-2.4.0 google-cloud-dlp-3.6.1 google-cloud-language-2.3.1 google-cloud-pubsub-2.9.0 google-cloud-pubsublite-1.4.0 google-cloud-recommendations-ai-0.2.0 google-cloud-spanner-3.12.0 google-cloud-videointelligence-2.5.1 google-cloud-vision-2.6.3 google-crc32c-1.3.0 google-pasta-0.2.0 google-resumable-media-2.3.1 googleapis-common-protos-1.55.0 grpc-google-iam-v1-0.12.3 grpcio-1.44.0 grpcio-gcp-0.2.2 grpcio-status-1.44.0 h5py-3.6.0 hdfs-2.6.0 httplib2-0.19.1 idna-3.3 importlib-metadata-4.11.2 keras-2.7.0 keras-preprocessing-1.1.2 libclang-13.0.0 libcst-0.4.1 markdown-3.3.6 mypy-extensions-0.4.3 numpy-1.21.5 oauth2client-4.1.3 oauthlib-3.2.0 opt-einsum-3.3.0 orjson-3.6.7 overrides-6.1.0 packaging-21.3 pandas-1.3.5 proto-plus-1.20.3 protobuf-3.19.4 pyarrow-6.0.1 pyasn1-0.4.8 pyasn1-modules-0.2.8 pydot-1.4.2 pymongo-3.12.3 pyparsing-2.4.7 python-dateutil-2.8.2 pytz-2021.3 pyyaml-6.0 requests-2.27.1 requests-oauthlib-1.3.1 rsa-4.8 setuptools-60.9.3 six-1.16.0 tensorboard-2.8.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.7.1 tensorflow-estimator-2.7.0 tensorflow-io-gcs-filesystem-0.24.0 tensorflow-metadata-1.6.0 tensorflow-serving-api-2.7.0 tensorflow-transform-1.6.0 termcolor-1.1.0 tfx-bsl-1.6.0 typing-extensions-4.1.1 typing-inspect-0.7.1 typing-utils-0.1.0 uritemplate-4.1.1 urllib3-1.26.8 werkzeug-2.0.3 wheel-0.37.1 wrapt-1.13.3 zipp-3.7.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --ignore-installed tensorflow-transform==1.6.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29be7b0b-52c2-42be-8d3d-691298eabc97",
   "metadata": {},
   "source": [
    "You can ingnore the dependency resolver errors. Confirm the final message starting with \"Successfully installed ...\"\n",
    "\n",
    "**Now you have to restart kernel from the menu bar: \"Kernel\" -> \"Restart Kernel\".**\n",
    "\n",
    "After restarting the kernel, you can resume the code execution from the next cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdaa4984-6cee-4d54-a588-a7b6f9f9bcb6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Confirm the installed packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ad43b57-7e4b-410e-b354-5dab0c439b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apache-beam                           2.36.0\n",
      "tensorflow                            2.7.1\n",
      "tensorflow-cloud                      0.1.16\n",
      "tensorflow-datasets                   4.4.0\n",
      "tensorflow-estimator                  2.7.0\n",
      "tensorflow-hub                        0.12.0\n",
      "tensorflow-io                         0.21.0\n",
      "tensorflow-io-gcs-filesystem          0.24.0\n",
      "tensorflow-metadata                   1.6.0\n",
      "tensorflow-probability                0.14.1\n",
      "tensorflow-serving-api                2.7.0\n",
      "tensorflow-transform                  1.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep -E '(tensorflow|beam)'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f472115-8b24-4c48-af9a-8c08627d50f6",
   "metadata": {},
   "source": [
    "### Create setup.py to install packages to Dataflow containers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5475f8c-e551-4281-bd76-9c3fee920474",
   "metadata": {},
   "source": [
    "This is used to install additional packages to Dataflow worker containers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f021734a-0439-4a5a-87e3-d9209ba15ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile setup.py\n",
    "import setuptools\n",
    "\n",
    "setuptools.setup(\n",
    "    install_requires=['tensorflow-transform==1.6.0'],\n",
    "    packages=setuptools.find_packages(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0db8027-1722-43ce-a0ac-b12aec7250d5",
   "metadata": {},
   "source": [
    "### Set global flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a81e0009-101a-4fb6-98c1-6c8181d6ab31",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = 'your-project'\n",
    "BUCKET = 'your-project-babyweight'\n",
    "REGION = 'us-central1'\n",
    "ROOT_DIR = 'babyweight_tft'\n",
    "\n",
    "RUN_LOCAL = False # if True, the DirectRunner is used, else DataflowRunner\n",
    "DATA_SIZE = 10000 # number of records to be retrieved from BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcec29c0-1d9e-4024-97a0-a6d5eaf188e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['REGION'] = REGION\n",
    "os.environ['ROOT_DIR'] = ROOT_DIR\n",
    "os.environ['RUN_LOCAL'] = str(RUN_LOCAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b668941a-98ab-4857-9e3d-8398adfd5391",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n",
      "Updated property [compute/region].\n",
      "Creating gs://your-project-babyweight/...\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gcloud config set project $PROJECT\n",
    "gcloud config set compute/region $REGION\n",
    "if ! gsutil ls | grep -q gs://${BUCKET}/; then\n",
    "    gsutil mb -l ${REGION} gs://${BUCKET}\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafc6791-37bd-48be-9b7f-9219bdf742fc",
   "metadata": {},
   "source": [
    "### Import required packages and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7d7e950-1ca2-44c4-8e35-1919ba29078a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "\n",
    "import apache_beam as beam\n",
    "import tensorflow.compat.v2 as tf\n",
    "import tensorflow_transform as tft\n",
    "\n",
    "import tensorflow_transform.beam as tft_beam\n",
    "from tensorflow_transform.tf_metadata import dataset_metadata\n",
    "from tensorflow_transform.tf_metadata import schema_utils\n",
    "from tensorflow_transform.beam.tft_beam_io import transform_fn_io\n",
    "from tfx_bsl.coders import example_coder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab0e492-4e81-464c-9881-ac3b4b26101e",
   "metadata": {},
   "source": [
    "### Define raw input data and their metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15ae75e0-ff15-4c34-88b1-7a93cab9f1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORICAL_FEATURE_NAMES = ['is_male', 'mother_race']\n",
    "NUMERIC_FEATURE_NAMES = ['mother_age', 'plurality', 'gestation_weeks']\n",
    "TARGET_FEATURE_NAME = 'weight_pounds'\n",
    "\n",
    "def create_raw_metadata():  \n",
    "\n",
    "    feature_spec = dict(\n",
    "        [(name, tf.io.FixedLenFeature([], tf.string)) for name in CATEGORICAL_FEATURE_NAMES] +\n",
    "        [(name, tf.io.FixedLenFeature([], tf.float32)) for name in NUMERIC_FEATURE_NAMES] +\n",
    "        [(TARGET_FEATURE_NAME, tf.io.FixedLenFeature([], tf.float32))])\n",
    "\n",
    "    raw_metadata = dataset_metadata.DatasetMetadata(\n",
    "        schema_utils.schema_from_feature_spec(feature_spec))\n",
    "    \n",
    "    return raw_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c832e6ae-a910-490c-8c0c-0b7c6a9af577",
   "metadata": {},
   "source": [
    "The metadata contains feature schema in the protobuf format as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34d17003-4fc5-4b5b-b21d-a0b3f4a22ac3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature {\n",
       "  name: \"gestation_weeks\"\n",
       "  type: FLOAT\n",
       "  presence {\n",
       "    min_fraction: 1.0\n",
       "  }\n",
       "  shape {\n",
       "  }\n",
       "}\n",
       "feature {\n",
       "  name: \"is_male\"\n",
       "  type: BYTES\n",
       "  presence {\n",
       "    min_fraction: 1.0\n",
       "  }\n",
       "  shape {\n",
       "  }\n",
       "}\n",
       "feature {\n",
       "  name: \"mother_age\"\n",
       "  type: FLOAT\n",
       "  presence {\n",
       "    min_fraction: 1.0\n",
       "  }\n",
       "  shape {\n",
       "  }\n",
       "}\n",
       "feature {\n",
       "  name: \"mother_race\"\n",
       "  type: BYTES\n",
       "  presence {\n",
       "    min_fraction: 1.0\n",
       "  }\n",
       "  shape {\n",
       "  }\n",
       "}\n",
       "feature {\n",
       "  name: \"plurality\"\n",
       "  type: FLOAT\n",
       "  presence {\n",
       "    min_fraction: 1.0\n",
       "  }\n",
       "  shape {\n",
       "  }\n",
       "}\n",
       "feature {\n",
       "  name: \"weight_pounds\"\n",
       "  type: FLOAT\n",
       "  presence {\n",
       "    min_fraction: 1.0\n",
       "  }\n",
       "  shape {\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_raw_metadata().schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12bec8e-1cc5-416f-b57a-92facbe68c57",
   "metadata": {},
   "source": [
    "### Define source query and source cleanup function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c422ff3-df95-4807-8ab0-2888efb90795",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_source_query(step, data_size):\n",
    "    \n",
    "    train_size = data_size * 0.7\n",
    "    eval_size = data_size * 0.3\n",
    "    \n",
    "    query = \"\"\"\n",
    "    SELECT\n",
    "      ROUND(weight_pounds,1) AS weight_pounds,\n",
    "      is_male,\n",
    "      mother_age,\n",
    "      mother_race,\n",
    "      plurality,\n",
    "      gestation_weeks,\n",
    "      FARM_FINGERPRINT( \n",
    "        CONCAT(\n",
    "          COALESCE(CAST(weight_pounds AS STRING), 'NA'),\n",
    "          COALESCE(CAST(is_male AS STRING),'NA'),\n",
    "          COALESCE(CAST(mother_age AS STRING),'NA'),\n",
    "          COALESCE(CAST(mother_race AS STRING),'NA'),\n",
    "          COALESCE(CAST(plurality AS STRING), 'NA'),\n",
    "          COALESCE(CAST(gestation_weeks AS STRING),'NA')\n",
    "          )\n",
    "        ) AS key\n",
    "        FROM\n",
    "          publicdata.samples.natality\n",
    "        WHERE year > 2000\n",
    "        AND weight_pounds > 0\n",
    "        AND mother_age > 0\n",
    "        AND plurality > 0\n",
    "        AND gestation_weeks > 0\n",
    "        AND month > 0\n",
    "    \"\"\"\n",
    "    \n",
    "    if step == 'train':\n",
    "        source_query = 'SELECT * FROM ({}) WHERE MOD(key, 100) < 70 LIMIT {}'.format(query, int(train_size))\n",
    "    else:\n",
    "        source_query = 'SELECT * FROM ({}) WHERE MOD(key, 100) >= 70 LIMIT {}'.format(query, int(eval_size))\n",
    "    \n",
    "    return source_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58ec77ef-0c60-4a0b-baca-f90409c6ca77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_bq_row(bq_row):\n",
    "\n",
    "    # modify opaque numeric race code into human-readable data\n",
    "    races = dict(zip([1,2,3,4,5,6,7,18,28,39,48],\n",
    "                     ['White', 'Black', 'American Indian', 'Chinese', \n",
    "                      'Japanese', 'Hawaiian', 'Filipino',\n",
    "                      'Asian Indian', 'Korean', 'Samaon', 'Vietnamese']))\n",
    "    result = {} \n",
    "    \n",
    "    for feature_name in bq_row.keys():\n",
    "        if isinstance(bq_row[feature_name], bool):\n",
    "            result[feature_name] = str(bq_row[feature_name])\n",
    "        else:\n",
    "            result[feature_name] = bq_row[feature_name]\n",
    "\n",
    "    if 'mother_race' in bq_row and bq_row['mother_race'] in races:\n",
    "        result['mother_race'] = races[bq_row['mother_race']]\n",
    "    else:\n",
    "        result['mother_race'] = 'Unknown'\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d1ca02-d66b-4a90-bd00-2a17311dd1ad",
   "metadata": {},
   "source": [
    "The output from the cleanup function `prep_bq_row` is used as raw input data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35c4d29-da6d-45e5-be85-7680f822340a",
   "metadata": {},
   "source": [
    "## 2. Define data transformation pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f53aeab-2053-4f31-b312-4de89ab7d0b9",
   "metadata": {},
   "source": [
    "First, we define component functions in the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b76ff2-93e5-48ff-953f-a24ddd81d8c9",
   "metadata": {},
   "source": [
    "### Read and clean from source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee9850b3-9627-4982-a075-54a3209986a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_bq(pipeline, step, data_size):\n",
    "    \n",
    "    source_query = get_source_query(step, data_size)\n",
    "    raw_data = (\n",
    "        pipeline\n",
    "        | '{} - Read Data from BigQuery'.format(step) >> beam.io.Read(\n",
    "            beam.io.BigQuerySource(query=source_query, use_standard_sql=True))\n",
    "        | '{} - Clean up Data'.format(step) >> beam.Map(prep_bq_row)\n",
    "    )\n",
    "\n",
    "    # Assosiate the metadata to the raw input data. The metadata are used for the transformation.\n",
    "    raw_metadata = create_raw_metadata()\n",
    "    raw_dataset = (raw_data, raw_metadata)\n",
    "    return raw_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdeb3fd9-5067-4bd0-a6d2-0165aa0af2aa",
   "metadata": {},
   "source": [
    "### tf.Transform preprocess_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b9f997-2ff6-4e6f-b160-dd942f75b40d",
   "metadata": {},
   "source": [
    "This function defines the data transformation against raw input data. This will be used by the data transformation pipeline, and also embeded in the exported model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc48b270-c1b9-429a-acf9-b6ba3e407493",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_fn(input_features):\n",
    "\n",
    "    # explicitly import packages here as the apache beam fails to serialize them from the global context.\n",
    "    import tensorflow.compat.v2 as tf\n",
    "    import tensorflow_transform as tft\n",
    "\n",
    "    output_features = {}\n",
    "\n",
    "    # target feature\n",
    "    output_features['weight_pounds'] = input_features['weight_pounds']\n",
    "\n",
    "    # normalisation\n",
    "    output_features['mother_age_normalized'] = tft.scale_to_z_score(input_features['mother_age'])\n",
    "    \n",
    "    # scaling\n",
    "    output_features['gestation_weeks_scaled'] =  tft.scale_to_0_1(input_features['gestation_weeks'])\n",
    "    \n",
    "    # bucketisation based on quantiles\n",
    "    output_features['mother_age_bucketized'] = tft.bucketize(input_features['mother_age'], num_buckets=5)\n",
    "    \n",
    "    # you can compute new features based on custom formulas\n",
    "    output_features['mother_age_log'] = tf.math.log(input_features['mother_age'])\n",
    "    \n",
    "    # or create flags/indicators\n",
    "    is_multiple = tf.as_string(input_features['plurality'] > tf.constant(1.0))\n",
    "    \n",
    "    # convert categorical features to indexed vocab\n",
    "    output_features['mother_race_index'] = tft.compute_and_apply_vocabulary(input_features['mother_race'], vocab_filename='mother_race')\n",
    "    output_features['is_male_index'] = tft.compute_and_apply_vocabulary(input_features['is_male'], vocab_filename='is_male')\n",
    "    output_features['is_multiple_index'] = tft.compute_and_apply_vocabulary(is_multiple, vocab_filename='is_multiple')\n",
    "    \n",
    "    return output_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449e0d4a-c5c1-4ff3-88d7-1c12f1e2fc60",
   "metadata": {},
   "source": [
    "### Analyze and transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba346d9-64c7-41e2-8d64-44fbf87c3156",
   "metadata": {},
   "source": [
    "This is applied to the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67798d22-11fa-4c54-99f5-e22aeef9efbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_and_transform(raw_dataset, step):\n",
    "    \n",
    "    transformed_dataset, transform_fn = (\n",
    "        raw_dataset \n",
    "        | '{} - Analyze & Transform'.format(step) >> tft_beam.AnalyzeAndTransformDataset(\n",
    "            preprocess_fn, output_record_batches=True)\n",
    "    )\n",
    "    \n",
    "    return transformed_dataset, transform_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0327c9-0d7d-42a6-9cfe-0e3fbf73d139",
   "metadata": {},
   "source": [
    "### Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8420438-cb01-4d43-9bcf-8fc7f83728b0",
   "metadata": {},
   "source": [
    "This is applied to the evaluation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "795b76f5-b538-4b9a-aa92-65e8b267f6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(raw_dataset, transform_fn, step):\n",
    "    \n",
    "    transformed_dataset = (\n",
    "        (raw_dataset, transform_fn) \n",
    "        | '{} - Transform'.format(step) >> tft_beam.TransformDataset(output_record_batches=True)\n",
    "    )\n",
    "    \n",
    "    return transformed_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de612fca-4c2d-4192-adbb-74548b55965e",
   "metadata": {},
   "source": [
    "### Write tfrecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62cbee0b-109e-4e93-a42a-5f42573f3394",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_tfrecords(transformed_dataset, location, step):\n",
    "    from tfx_bsl.coders import example_coder\n",
    "\n",
    "    transformed_data, transformed_metadata = transformed_dataset\n",
    "    (\n",
    "        transformed_data\n",
    "        | '{} - Encode Transformed Data'.format(step) >> beam.FlatMapTuple(\n",
    "                            lambda batch, _: example_coder.RecordBatchToExamples(batch))\n",
    "        | '{} - Write Transformed Data'.format(step) >> beam.io.WriteToTFRecord(\n",
    "                            file_path_prefix=os.path.join(location,'{}'.format(step)),\n",
    "                            file_name_suffix='.tfrecords')\n",
    "    )  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7663fb1-2280-40f1-b114-b333594fc2b8",
   "metadata": {},
   "source": [
    "### Write text records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f2978df4-afa1-4410-9876-b6e2e4e4b40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_text(dataset, location, step):\n",
    "    \n",
    "    data, _ = dataset\n",
    "    (\n",
    "        data \n",
    "        | '{} - WriteData'.format(step) >> beam.io.WriteToText(\n",
    "            file_path_prefix=os.path.join(location,'{}'.format(step)),\n",
    "            file_name_suffix='.txt')\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d719d45-e5c4-4867-bbb8-29629df4872d",
   "metadata": {},
   "source": [
    "### Write transformation artefacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "167da7a2-00a1-446e-a420-2bda7229b077",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_transform_artefacts(transform_fn, location):\n",
    "    \n",
    "    (\n",
    "        transform_fn \n",
    "        | 'Write Transform Artefacts' >> transform_fn_io.WriteTransformFn(location)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df0db43-3fd8-4681-b772-82eb236c2833",
   "metadata": {},
   "source": [
    "Now we can construct a pipeline by combining components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25384b1d-0f1e-4980-bd69-3486f69b8c1b",
   "metadata": {},
   "source": [
    "### Construct data transformation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec745a34-7031-491f-8e70-a1557af0fbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_transformation_pipeline(args):\n",
    "    \n",
    "    pipeline_options = beam.pipeline.PipelineOptions(flags=[], **args)\n",
    "    \n",
    "    runner = args['runner']\n",
    "    data_size = args['data_size']\n",
    "    transformed_data_location = args['transformed_data_location']\n",
    "    transform_artefact_location = args['transform_artefact_location']\n",
    "    temporary_dir = args['temporary_dir']\n",
    "    debug = args['debug']\n",
    "    \n",
    "    print(\"Sample data size: {}\".format(data_size))\n",
    "    print(\"Sink transformed data files location: {}\".format(transformed_data_location))\n",
    "    print(\"Sink transform artefact location: {}\".format(transform_artefact_location))\n",
    "    print(\"Temporary directory: {}\".format(temporary_dir))\n",
    "    print(\"Runner: {}\".format(runner))\n",
    "    print(\"Debug enabled: {}\".format(debug))\n",
    "\n",
    "    with beam.Pipeline(runner, options=pipeline_options) as pipeline:\n",
    "        with tft_beam.Context(temporary_dir):\n",
    "            \n",
    "            # Preprocess train data\n",
    "            step = 'train'\n",
    "            # Read raw train data from BQ\n",
    "            raw_train_dataset = read_from_bq(pipeline, step, data_size)\n",
    "            # Analyze and transform raw_train_dataset \n",
    "            transformed_train_dataset, transform_fn = analyze_and_transform(raw_train_dataset, step)\n",
    "            # Write transformed train data to sink as tfrecords\n",
    "            write_tfrecords(transformed_train_dataset, transformed_data_location, step)\n",
    "            \n",
    "            # Preprocess evaluation data\n",
    "            step = 'eval'\n",
    "            # Read raw eval data from BQ\n",
    "            raw_eval_dataset = read_from_bq(pipeline, step, data_size)\n",
    "            # Transform eval data based on produced transform_fn\n",
    "            transformed_eval_dataset = transform(raw_eval_dataset, transform_fn, step)\n",
    "            # Write transformed eval data to sink as tfrecords\n",
    "            write_tfrecords(transformed_eval_dataset, transformed_data_location, step)\n",
    "            \n",
    "            # Write transformation artefacts \n",
    "            write_transform_artefacts(transform_fn, transform_artefact_location)\n",
    "\n",
    "            # (Optional) for debugging, write transformed data as text \n",
    "            step = 'debug'\n",
    "            # Wwrite transformed train data as text if debug enabled\n",
    "            if debug == True:\n",
    "                write_text(transformed_train_dataset, transformed_data_location, step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab0a5d2-367a-46c9-adfc-47d2f41a699e",
   "metadata": {},
   "source": [
    "## 3. Execute transformation pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91c9768-d624-442d-9c05-9a040ca527cd",
   "metadata": {},
   "source": [
    "### Set pipeline parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "27db9263-51c8-4484-891b-4910c3090f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "OUTPUT_DIR = \"gs://{}/{}\".format(BUCKET, ROOT_DIR)\n",
    "TRANSFORM_ARTEFACTS_DIR = os.path.join(OUTPUT_DIR, 'transform')\n",
    "TRANSFORMED_DATA_DIR = os.path.join(OUTPUT_DIR, 'transformed')\n",
    "TEMP_DIR = os.path.join(OUTPUT_DIR, 'tmp')\n",
    "\n",
    "if RUN_LOCAL:\n",
    "    runner = 'DirectRunner'\n",
    "else:\n",
    "    runner = 'DataflowRunner'\n",
    "\n",
    "job_name = 'preprocess-babweight-data-tft-{}'.format(datetime.utcnow().strftime('%y%m%d-%H%M%S'))\n",
    "\n",
    "args = {\n",
    "    'job_name': job_name,\n",
    "    'runner': runner,\n",
    "    'data_size': DATA_SIZE,\n",
    "    'transformed_data_location':  TRANSFORMED_DATA_DIR,\n",
    "    'transform_artefact_location':  TRANSFORM_ARTEFACTS_DIR,\n",
    "    'temporary_dir': TEMP_DIR,\n",
    "    'debug': False,\n",
    "    \n",
    "    'project': PROJECT,\n",
    "    'region': REGION,\n",
    "    'staging_location': os.path.join(OUTPUT_DIR, 'staging'),\n",
    "    'temp_location': os.path.join(OUTPUT_DIR, 'tmp'),\n",
    "    'worker_machine_type': 'n1-standard-1',\n",
    "    'max_num_workers': 3,\n",
    "    'setup_file': './setup.py', # requirements_file doesn't work as tft provides only wheel pkg.\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c615e594-8263-4a51-91c2-71b79f3360c4",
   "metadata": {},
   "source": [
    "### Run pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c36ecf3d-4265-46ff-86db-02508079085b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:8: BeamDeprecationWarning: BigQuerySource is deprecated since 2.25.0. Use ReadFromBigQuery instead.\n",
      "  \n",
      "WARNING:apache_beam.runners.interactive.interactive_environment:Dependencies required for Interactive Beam PCollection visualization are not available, please use: `pip install apache-beam[interactive]` to install necessary dependencies to enable all data visualization features.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching DataflowRunner job preprocess-babweight-data-tft-220304-074631 ... hang on\n",
      "\n",
      "Sample data size: 10000\n",
      "Sink transformed data files location: gs://your-project-babyweight/babyweight_tft/transformed\n",
      "Sink transform artefact location: gs://your-project-babyweight/babyweight_tft/transform\n",
      "Temporary directory: gs://your-project-babyweight/babyweight_tft/tmp\n",
      "Runner: DataflowRunner\n",
      "Debug enabled: False\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (typeof window.interactive_beam_jquery == 'undefined') {\n",
       "          var jqueryScript = document.createElement('script');\n",
       "          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n",
       "          jqueryScript.type = 'text/javascript';\n",
       "          jqueryScript.onload = function() {\n",
       "            var datatableScript = document.createElement('script');\n",
       "            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n",
       "            datatableScript.type = 'text/javascript';\n",
       "            datatableScript.onload = function() {\n",
       "              window.interactive_beam_jquery = jQuery.noConflict(true);\n",
       "              window.interactive_beam_jquery(document).ready(function($){\n",
       "                \n",
       "              });\n",
       "            }\n",
       "            document.head.appendChild(datatableScript);\n",
       "          };\n",
       "          document.head.appendChild(jqueryScript);\n",
       "        } else {\n",
       "          window.interactive_beam_jquery(document).ready(function($){\n",
       "            \n",
       "          });\n",
       "        }"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery.py:2437: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported\n",
      "  temp_location = pcoll.pipeline.options.view_as(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:You are passing instance dicts and DatasetMetadata to TFT which will not provide optimal performance. Consider following the TFT guide to upgrade to the TFXIO format (Apache Arrow RecordBatch).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:You are passing instance dicts and DatasetMetadata to TFT which will not provide optimal performance. Consider following the TFT guide to upgrade to the TFXIO format (Apache Arrow RecordBatch).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_transform/tf_utils.py:289: Tensor.experimental_ref (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use ref() instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-04 07:46:38.062071: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2022-03-04 07:46:38.062127: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-03-04 07:46:38.062165: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (tensorflow-2-7-20220304-163425): /proc/driver/nvidia/version does not exist\n",
      "2022-03-04 07:46:38.062514: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_transform/tf_utils.py:289: Tensor.experimental_ref (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use ref() instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:You are passing instance dicts and DatasetMetadata to TFT which will not provide optimal performance. Consider following the TFT guide to upgrade to the TFXIO format (Apache Arrow RecordBatch).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:You are passing instance dicts and DatasetMetadata to TFT which will not provide optimal performance. Consider following the TFT guide to upgrade to the TFXIO format (Apache Arrow RecordBatch).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:You are passing instance dicts and DatasetMetadata to TFT which will not provide optimal performance. Consider following the TFT guide to upgrade to the TFXIO format (Apache Arrow RecordBatch).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:You are passing instance dicts and DatasetMetadata to TFT which will not provide optimal performance. Consider following the TFT guide to upgrade to the TFXIO format (Apache Arrow RecordBatch).\n",
      "warning: check: missing required meta-data: name, url\n",
      "\n",
      "warning: check: missing meta-data: either (author and author_email) or (maintainer and maintainer_email) should be supplied\n",
      "\n",
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding invalid overrides: {'data_size': 10000, 'transformed_data_location': 'gs://your-project-babyweight/babyweight_tft/transformed', 'transform_artefact_location': 'gs://your-project-babyweight/babyweight_tft/transform', 'temporary_dir': 'gs://your-project-babyweight/babyweight_tft/tmp', 'debug': False, 'worker_machine_type': 'n1-standard-1'}\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding invalid overrides: {'data_size': 10000, 'transformed_data_location': 'gs://your-project-babyweight/babyweight_tft/transformed', 'transform_artefact_location': 'gs://your-project-babyweight/babyweight_tft/transform', 'temporary_dir': 'gs://your-project-babyweight/babyweight_tft/tmp', 'debug': False, 'worker_machine_type': 'n1-standard-1'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "try: \n",
    "    tf.io.gfile.rmtree(TRANSFORMED_DATA_DIR)\n",
    "    tf.io.gfile.rmtree(TRANSFORM_ARTEFACTS_DIR)\n",
    "    tf.io.gfile.rmtree(TEMP_DIR)\n",
    "    print('previous transformation files deleted!')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print('Launching {} job {} ... hang on'.format(runner, job_name))\n",
    "print('')\n",
    "run_transformation_pipeline(args)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a60e48a-7f40-4961-8d5a-29bbe579473f",
   "metadata": {},
   "source": [
    "### Explore the produced artefacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4017c92b-4b2d-48da-bfac-c3c5ab8fad4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformed data:\n",
      "gs://your-project-babyweight/babyweight_tft/transformed/eval-00000-of-00001.tfrecords\n",
      "gs://your-project-babyweight/babyweight_tft/transformed/train-00000-of-00004.tfrecords\n",
      "gs://your-project-babyweight/babyweight_tft/transformed/train-00001-of-00004.tfrecords\n",
      "gs://your-project-babyweight/babyweight_tft/transformed/train-00002-of-00004.tfrecords\n",
      "gs://your-project-babyweight/babyweight_tft/transformed/train-00003-of-00004.tfrecords\n",
      "\n",
      "transformed metadata:\n",
      "gs://your-project-babyweight/babyweight_tft/transform/transformed_metadata/\n",
      "gs://your-project-babyweight/babyweight_tft/transform/transformed_metadata/asset_map\n",
      "gs://your-project-babyweight/babyweight_tft/transform/transformed_metadata/schema.pbtxt\n",
      "\n",
      "transform artefact:\n",
      "gs://your-project-babyweight/babyweight_tft/transform/transform_fn/\n",
      "gs://your-project-babyweight/babyweight_tft/transform/transform_fn/saved_model.pb\n",
      "gs://your-project-babyweight/babyweight_tft/transform/transform_fn/assets/\n",
      "gs://your-project-babyweight/babyweight_tft/transform/transform_fn/variables/\n",
      "\n",
      "transform assets:\n",
      "gs://your-project-babyweight/babyweight_tft/transform/transform_fn/assets/\n",
      "gs://your-project-babyweight/babyweight_tft/transform/transform_fn/assets/is_male\n",
      "gs://your-project-babyweight/babyweight_tft/transform/transform_fn/assets/is_multiple\n",
      "gs://your-project-babyweight/babyweight_tft/transform/transform_fn/assets/mother_race\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "echo 'transformed data:' \n",
    "gsutil ls gs://${BUCKET}/${ROOT_DIR}/transformed \n",
    "echo ''\n",
    "\n",
    "echo 'transformed metadata:'  \n",
    "gsutil ls gs://${BUCKET}/${ROOT_DIR}/transform/transformed_metadata \n",
    "echo ''\n",
    "\n",
    "echo 'transform artefact:'   \n",
    "gsutil ls gs://${BUCKET}/${ROOT_DIR}/transform/transform_fn \n",
    "echo ''\n",
    "\n",
    "echo 'transform assets:'\n",
    "gsutil ls gs://${BUCKET}/${ROOT_DIR}/transform/transform_fn/assets "
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-7.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-7:m87"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
