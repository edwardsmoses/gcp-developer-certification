{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "881462bc-b115-4b49-b963-e9e68f06be41",
   "metadata": {},
   "source": [
    "# Babyweight Estimation with Transformed Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9992a281-8ed0-4003-aa0e-226db8434799",
   "metadata": {},
   "source": [
    "## 1. Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24126cd5-8efa-4cef-9b51-617e341ea211",
   "metadata": {},
   "source": [
    "### Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcbf1652-be4e-4695-87f6-bf5262921c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-transform==1.6.0\n",
      "  Using cached tensorflow_transform-1.6.0-py3-none-any.whl (427 kB)\n",
      "Collecting numpy<2,>=1.16\n",
      "  Using cached numpy-1.21.5-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
      "Collecting tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,<2.8,>=1.15.5\n",
      "  Using cached tensorflow-2.7.1-cp37-cp37m-manylinux2010_x86_64.whl (495.0 MB)\n",
      "Collecting pyarrow<6,>=1\n",
      "  Using cached pyarrow-5.0.0-cp37-cp37m-manylinux2014_x86_64.whl (23.6 MB)\n",
      "Collecting tfx-bsl<1.7.0,>=1.6.0\n",
      "  Using cached tfx_bsl-1.6.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (19.1 MB)\n",
      "Collecting protobuf<4,>=3.13\n",
      "  Using cached protobuf-3.19.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "Collecting tensorflow-metadata<1.7.0,>=1.6.0\n",
      "  Using cached tensorflow_metadata-1.6.0-py3-none-any.whl (48 kB)\n",
      "Collecting apache-beam[gcp]<3,>=2.35\n",
      "  Using cached apache_beam-2.36.0-cp37-cp37m-manylinux2010_x86_64.whl (10.1 MB)\n",
      "Collecting pydot<2,>=1.2\n",
      "  Using cached pydot-1.4.2-py2.py3-none-any.whl (21 kB)\n",
      "Collecting absl-py<2.0.0,>=0.9\n",
      "  Using cached absl_py-1.0.0-py3-none-any.whl (126 kB)\n",
      "Collecting six\n",
      "  Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting cloudpickle<3,>=2.0.0\n",
      "  Using cached cloudpickle-2.0.0-py3-none-any.whl (25 kB)\n",
      "Collecting proto-plus<2,>=1.7.1\n",
      "  Using cached proto_plus-1.20.3-py3-none-any.whl (46 kB)\n",
      "Collecting dill<0.3.2,>=0.3.1.1\n",
      "  Using cached dill-0.3.1.1-py3-none-any.whl\n",
      "Collecting typing-extensions>=3.7.0\n",
      "  Using cached typing_extensions-4.1.1-py3-none-any.whl (26 kB)\n",
      "Collecting python-dateutil<3,>=2.8.0\n",
      "  Using cached python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "Collecting crcmod<2.0,>=1.7\n",
      "  Using cached crcmod-1.7-cp37-cp37m-linux_x86_64.whl\n",
      "Collecting httplib2<0.20.0,>=0.8\n",
      "  Using cached httplib2-0.19.1-py3-none-any.whl (95 kB)\n",
      "Collecting fastavro<2,>=0.21.4\n",
      "  Using cached fastavro-1.4.9-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
      "Collecting oauth2client<5,>=2.0.1\n",
      "  Using cached oauth2client-4.1.3-py2.py3-none-any.whl (98 kB)\n",
      "Collecting pytz>=2018.3\n",
      "  Using cached pytz-2021.3-py2.py3-none-any.whl (503 kB)\n",
      "Collecting requests<3.0.0,>=2.24.0\n",
      "  Using cached requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
      "Collecting pymongo<4.0.0,>=3.8.0\n",
      "  Using cached pymongo-3.12.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (508 kB)\n",
      "Collecting hdfs<3.0.0,>=2.1.0\n",
      "  Using cached hdfs-2.6.0-py3-none-any.whl (33 kB)\n",
      "Collecting grpcio<2,>=1.29.0\n",
      "  Using cached grpcio-1.44.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n",
      "Collecting orjson<4.0\n",
      "  Using cached orjson-3.6.7-cp37-cp37m-manylinux_2_24_x86_64.whl (255 kB)\n",
      "Collecting google-cloud-spanner<2,>=1.13.0\n",
      "  Using cached google_cloud_spanner-1.19.1-py2.py3-none-any.whl (255 kB)\n",
      "Collecting google-cloud-core<2,>=0.28.1\n",
      "  Using cached google_cloud_core-1.7.2-py2.py3-none-any.whl (28 kB)\n",
      "Collecting google-cloud-language<2,>=1.3.0\n",
      "  Using cached google_cloud_language-1.3.0-py2.py3-none-any.whl (83 kB)\n",
      "Collecting google-cloud-bigtable<2,>=0.31.1\n",
      "  Using cached google_cloud_bigtable-1.7.0-py2.py3-none-any.whl (267 kB)\n",
      "Collecting google-cloud-recommendations-ai<=0.2.0,>=0.1.0\n",
      "  Using cached google_cloud_recommendations_ai-0.2.0-py2.py3-none-any.whl (180 kB)\n",
      "Collecting google-cloud-pubsublite<2,>=1.2.0\n",
      "  Using cached google_cloud_pubsublite-1.4.0-py2.py3-none-any.whl (261 kB)\n",
      "Collecting google-cloud-vision<2,>=0.38.0\n",
      "  Using cached google_cloud_vision-1.0.0-py2.py3-none-any.whl (435 kB)\n",
      "Collecting google-cloud-dlp<4,>=3.0.0\n",
      "  Using cached google_cloud_dlp-3.6.1-py2.py3-none-any.whl (113 kB)\n",
      "Collecting google-cloud-videointelligence<2,>=1.8.0\n",
      "  Using cached google_cloud_videointelligence-1.16.1-py2.py3-none-any.whl (183 kB)\n",
      "Collecting google-cloud-datastore<2,>=1.8.0\n",
      "  Using cached google_cloud_datastore-1.15.3-py2.py3-none-any.whl (134 kB)\n",
      "Collecting google-cloud-bigquery<3,>=1.6.0\n",
      "  Using cached google_cloud_bigquery-2.34.1-py2.py3-none-any.whl (206 kB)\n",
      "Collecting google-cloud-pubsub<3,>=2.1.0\n",
      "  Using cached google_cloud_pubsub-2.9.0-py2.py3-none-any.whl (219 kB)\n",
      "Collecting grpcio-gcp<1,>=0.2.2\n",
      "  Using cached grpcio_gcp-0.2.2-py2.py3-none-any.whl (9.4 kB)\n",
      "Collecting google-auth<3,>=1.18.0\n",
      "  Using cached google_auth-2.6.0-py2.py3-none-any.whl (156 kB)\n",
      "Collecting google-apitools<0.5.32,>=0.5.31\n",
      "  Using cached google_apitools-0.5.31-py3-none-any.whl\n",
      "Collecting google-cloud-bigquery-storage>=2.6.3\n",
      "  Using cached google_cloud_bigquery_storage-2.12.0-py2.py3-none-any.whl (179 kB)\n",
      "Collecting cachetools<5,>=3.1.0\n",
      "  Using cached cachetools-4.2.4-py3-none-any.whl (10 kB)\n",
      "Collecting pyparsing>=2.1.4\n",
      "  Using cached pyparsing-3.0.7-py3-none-any.whl (98 kB)\n",
      "Collecting tensorboard~=2.6\n",
      "  Using cached tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting h5py>=2.9.0\n",
      "  Using cached h5py-3.6.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.1 MB)\n",
      "Collecting wrapt>=1.11.0\n",
      "  Using cached wrapt-1.13.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (79 kB)\n",
      "Collecting tensorflow-estimator<2.8,~=2.7.0rc0\n",
      "  Using cached tensorflow_estimator-2.7.0-py2.py3-none-any.whl (463 kB)\n",
      "Collecting keras<2.8,>=2.7.0rc0\n",
      "  Using cached keras-2.7.0-py2.py3-none-any.whl (1.3 MB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Using cached termcolor-1.1.0-py3-none-any.whl\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.21.0\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.24.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.1 MB)\n",
      "Collecting libclang>=9.0.1\n",
      "  Using cached libclang-13.0.0-py2.py3-none-manylinux1_x86_64.whl (14.5 MB)\n",
      "Collecting keras-preprocessing>=1.1.1\n",
      "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Collecting gast<0.5.0,>=0.2.1\n",
      "  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting wheel<1.0,>=0.32.0\n",
      "  Using cached wheel-0.37.1-py2.py3-none-any.whl (35 kB)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting flatbuffers<3.0,>=1.12\n",
      "  Using cached flatbuffers-2.0-py2.py3-none-any.whl (26 kB)\n",
      "Collecting googleapis-common-protos<2,>=1.52.0\n",
      "  Using cached googleapis_common_protos-1.55.0-py2.py3-none-any.whl (212 kB)\n",
      "Collecting pandas<2,>=1.0\n",
      "  Using cached pandas-1.3.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
      "Collecting tensorflow-serving-api!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,<3,>=1.15\n",
      "  Using cached tensorflow_serving_api-2.8.0-py2.py3-none-any.whl (37 kB)\n",
      "Collecting google-api-python-client<2,>=1.7.11\n",
      "  Using cached google_api_python_client-1.12.10-py2.py3-none-any.whl (61 kB)\n",
      "Collecting google-auth-httplib2>=0.0.3\n",
      "  Using cached google_auth_httplib2-0.1.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Collecting google-api-core<3dev,>=1.21.0\n",
      "  Using cached google_api_core-2.6.0-py2.py3-none-any.whl (114 kB)\n",
      "Collecting uritemplate<4dev,>=3.0.0\n",
      "  Using cached uritemplate-3.0.1-py2.py3-none-any.whl (15 kB)\n",
      "Collecting fasteners>=0.14\n",
      "  Using cached fasteners-0.17.3-py3-none-any.whl (18 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Using cached rsa-4.8-py3-none-any.whl (39 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting packaging>=14.3\n",
      "  Using cached packaging-21.3-py3-none-any.whl (40 kB)\n",
      "Collecting google-resumable-media<3.0dev,>=0.6.0\n",
      "  Using cached google_resumable_media-2.3.1-py2.py3-none-any.whl (76 kB)\n",
      "Collecting grpc-google-iam-v1<0.13dev,>=0.12.3\n",
      "  Using cached grpc_google_iam_v1-0.12.3-py3-none-any.whl\n",
      "Collecting google-api-core[grpc]<3.0.0dev,>=1.29.0\n",
      "  Using cached google_api_core-1.31.5-py2.py3-none-any.whl (93 kB)\n",
      "Collecting google-auth<3,>=1.18.0\n",
      "  Using cached google_auth-1.35.0-py2.py3-none-any.whl (152 kB)\n",
      "Collecting setuptools>=40.3.0\n",
      "  Using cached setuptools-60.9.3-py3-none-any.whl (1.1 MB)\n",
      "Collecting libcst>=0.3.10\n",
      "  Using cached libcst-0.4.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
      "Collecting overrides<7.0.0,>=6.0.1\n",
      "  Using cached overrides-6.1.0-py3-none-any.whl (14 kB)\n",
      "Collecting grpcio-status>=1.18.0\n",
      "  Using cached grpcio_status-1.44.0-py3-none-any.whl (10.0 kB)\n",
      "Collecting cached-property\n",
      "  Using cached cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Collecting docopt\n",
      "  Using cached docopt-0.6.2-py2.py3-none-any.whl\n",
      "Collecting pyparsing>=2.1.4\n",
      "  Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
      "Collecting pyasn1>=0.1.7\n",
      "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Collecting idna<4,>=2.5\n",
      "  Using cached idna-3.3-py3-none-any.whl (61 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Using cached certifi-2021.10.8-py2.py3-none-any.whl (149 kB)\n",
      "Collecting charset-normalizer~=2.0.0\n",
      "  Using cached charset_normalizer-2.0.12-py3-none-any.whl (39 kB)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Using cached urllib3-1.26.8-py2.py3-none-any.whl (138 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Using cached Markdown-3.3.6-py3-none-any.whl (97 kB)\n",
      "Collecting werkzeug>=0.11.15\n",
      "  Using cached Werkzeug-2.0.3-py3-none-any.whl (289 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Using cached tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Using cached tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "Collecting tensorflow-serving-api!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,<3,>=1.15\n",
      "  Using cached tensorflow_serving_api-2.7.0-py2.py3-none-any.whl (37 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting google-crc32c<2.0dev,>=1.0\n",
      "  Using cached google_crc32c-1.3.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38 kB)\n",
      "Collecting pyyaml>=5.2\n",
      "  Using cached PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
      "Collecting typing-inspect>=0.4.0\n",
      "  Using cached typing_inspect-0.7.1-py3-none-any.whl (8.4 kB)\n",
      "Collecting importlib-metadata>=4.4\n",
      "  Using cached importlib_metadata-4.11.2-py3-none-any.whl (17 kB)\n",
      "Collecting typing-utils>=0.0.3\n",
      "  Using cached typing_utils-0.1.0-py3-none-any.whl (10 kB)\n",
      "Collecting zipp>=0.5\n",
      "  Using cached zipp-3.7.0-py3-none-any.whl (5.3 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
      "Collecting mypy-extensions>=0.3.0\n",
      "  Using cached mypy_extensions-0.4.3-py2.py3-none-any.whl (4.5 kB)\n",
      "Installing collected packages: pyasn1, urllib3, six, setuptools, rsa, pyparsing, pyasn1-modules, protobuf, idna, charset-normalizer, certifi, cachetools, zipp, typing-extensions, requests, pytz, packaging, oauthlib, mypy-extensions, grpcio, googleapis-common-protos, google-auth, typing-inspect, requests-oauthlib, pyyaml, importlib-metadata, google-api-core, wheel, werkzeug, typing-utils, tensorboard-plugin-wit, tensorboard-data-server, proto-plus, numpy, markdown, libcst, httplib2, grpcio-gcp, grpc-google-iam-v1, google-crc32c, google-auth-oauthlib, docopt, cached-property, absl-py, wrapt, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard, python-dateutil, pymongo, pydot, pyarrow, overrides, orjson, opt-einsum, oauth2client, libclang, keras-preprocessing, keras, hdfs, h5py, grpcio-status, google-resumable-media, google-pasta, google-cloud-pubsub, google-cloud-core, gast, flatbuffers, fasteners, fastavro, dill, crcmod, cloudpickle, astunparse, uritemplate, tensorflow, google-cloud-vision, google-cloud-videointelligence, google-cloud-spanner, google-cloud-recommendations-ai, google-cloud-pubsublite, google-cloud-language, google-cloud-dlp, google-cloud-datastore, google-cloud-bigtable, google-cloud-bigquery-storage, google-cloud-bigquery, google-auth-httplib2, google-apitools, apache-beam, tensorflow-serving-api, tensorflow-metadata, pandas, google-api-python-client, tfx-bsl, tensorflow-transform\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "explainable-ai-sdk 1.3.2 requires xai-image-widget, which is not installed.\n",
      "tensorflow-io 0.21.0 requires tensorflow<2.7.0,>=2.6.0, but you have tensorflow 2.7.1 which is incompatible.\n",
      "tensorflow-io 0.21.0 requires tensorflow-io-gcs-filesystem==0.21.0, but you have tensorflow-io-gcs-filesystem 0.24.0 which is incompatible.\n",
      "pandas-profiling 3.0.0 requires tangled-up-in-unicode==0.1.0, but you have tangled-up-in-unicode 0.2.0 which is incompatible.\n",
      "numba 0.54.1 requires numpy<1.21,>=1.17, but you have numpy 1.21.5 which is incompatible.\n",
      "cloud-tpu-client 0.10 requires google-api-python-client==1.8.0, but you have google-api-python-client 1.12.10 which is incompatible.\u001b[0m\n",
      "Successfully installed absl-py-1.0.0 apache-beam-2.36.0 astunparse-1.6.3 cached-property-1.5.2 cachetools-4.2.4 certifi-2021.10.8 charset-normalizer-2.0.12 cloudpickle-2.0.0 crcmod-1.7 dill-0.3.1.1 docopt-0.6.2 fastavro-1.4.9 fasteners-0.17.3 flatbuffers-2.0 gast-0.4.0 google-api-core-2.3.2 google-api-python-client-2.33.0 google-apitools-0.5.31 google-auth-2.3.3 google-auth-httplib2-0.1.0 google-auth-oauthlib-0.4.6 google-cloud-bigquery-2.34.1 google-cloud-bigquery-storage-2.12.0 google-cloud-bigtable-2.4.0 google-cloud-core-2.2.1 google-cloud-datastore-2.4.0 google-cloud-dlp-3.6.1 google-cloud-language-2.3.1 google-cloud-pubsub-2.9.0 google-cloud-pubsublite-1.4.0 google-cloud-recommendations-ai-0.2.0 google-cloud-spanner-3.12.0 google-cloud-videointelligence-2.5.1 google-cloud-vision-2.6.3 google-crc32c-1.3.0 google-pasta-0.2.0 google-resumable-media-2.3.1 googleapis-common-protos-1.55.0 grpc-google-iam-v1-0.12.3 grpcio-1.44.0 grpcio-gcp-0.2.2 grpcio-status-1.44.0 h5py-3.6.0 hdfs-2.6.0 httplib2-0.19.1 idna-3.3 importlib-metadata-4.11.2 keras-2.7.0 keras-preprocessing-1.1.2 libclang-13.0.0 libcst-0.4.1 markdown-3.3.6 mypy-extensions-0.4.3 numpy-1.21.5 oauth2client-4.1.3 oauthlib-3.2.0 opt-einsum-3.3.0 orjson-3.6.7 overrides-6.1.0 packaging-21.3 pandas-1.3.5 proto-plus-1.20.3 protobuf-3.19.4 pyarrow-6.0.1 pyasn1-0.4.8 pyasn1-modules-0.2.8 pydot-1.4.2 pymongo-3.12.3 pyparsing-2.4.7 python-dateutil-2.8.2 pytz-2021.3 pyyaml-6.0 requests-2.27.1 requests-oauthlib-1.3.1 rsa-4.8 setuptools-60.9.3 six-1.16.0 tensorboard-2.8.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.7.1 tensorflow-estimator-2.7.0 tensorflow-io-gcs-filesystem-0.24.0 tensorflow-metadata-1.6.0 tensorflow-serving-api-2.7.0 tensorflow-transform-1.6.0 termcolor-1.1.0 tfx-bsl-1.6.0 typing-extensions-4.1.1 typing-inspect-0.7.1 typing-utils-0.1.0 uritemplate-4.1.1 urllib3-1.26.8 werkzeug-2.0.3 wheel-0.37.1 wrapt-1.13.3 zipp-3.7.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --ignore-installed tensorflow-transform==1.6.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c174a02-5538-4038-bf1d-8f325172cb01",
   "metadata": {},
   "source": [
    "You can ingnore the dependency resolver errors. Confirm the final message starting with \"Successfully installed ...\"\n",
    "\n",
    "**Now you have to restart kernel from the menu bar: \"Kernel\" -> \"Restart Kernel\".**\n",
    "\n",
    "After restarting the kernel, you can resume the code execution from the next cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36093fa6-c6a9-4556-966d-a8a56121a62b",
   "metadata": {},
   "source": [
    "### Confirm the installed packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46b3d806-7153-47e7-a5a2-b763d9763b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apache-beam                           2.36.0\n",
      "tensorflow                            2.7.1\n",
      "tensorflow-cloud                      0.1.16\n",
      "tensorflow-datasets                   4.4.0\n",
      "tensorflow-estimator                  2.7.0\n",
      "tensorflow-hub                        0.12.0\n",
      "tensorflow-io                         0.21.0\n",
      "tensorflow-io-gcs-filesystem          0.24.0\n",
      "tensorflow-metadata                   1.6.0\n",
      "tensorflow-probability                0.14.1\n",
      "tensorflow-serving-api                2.7.0\n",
      "tensorflow-transform                  1.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep -E '(tensorflow|beam)'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83602189-f2af-4bf6-bf5f-7c73f4c6c1af",
   "metadata": {},
   "source": [
    "### Set global flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08532dec-9327-4925-a547-6c6aedaa84e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = 'your-project'\n",
    "BUCKET = 'your-project-babyweight'\n",
    "REGION = 'us-central1'\n",
    "ROOT_DIR = 'babyweight_tft'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c6ce10f-6be6-4a1f-a8a1-a549480178aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['REGION'] = REGION\n",
    "os.environ['ROOT_DIR'] = ROOT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f0995fc-81e4-41ab-adef-b4cd46cdd947",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = 'gs://{}/{}'.format(BUCKET,ROOT_DIR)\n",
    "TRANSFORM_ARTEFACTS_DIR = os.path.join(OUTPUT_DIR,'transform')\n",
    "TRANSFORMED_DATA_DIR = os.path.join(OUTPUT_DIR,'transformed')\n",
    "TEMP_DIR = os.path.join(OUTPUT_DIR, 'tmp')\n",
    "MODELS_DIR = os.path.join(OUTPUT_DIR,'models')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c43032-1b62-429c-89e0-1f86fd536e46",
   "metadata": {},
   "source": [
    "### Import required packages and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "361a3a69-d9e1-4dfd-8260-2541bb356be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, os\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "\n",
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f1c7ad-9d5c-4737-bc8c-0c0f9594cb0d",
   "metadata": {},
   "source": [
    "## 2. Define deep and wide regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb9e128-2849-4b90-a01f-2b83119767c2",
   "metadata": {},
   "source": [
    "### Check features in the transformed data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51032e84-60cc-479e-8a76-0bb704beb7c6",
   "metadata": {},
   "source": [
    "You can use these features (except the target feature) as an input to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b446898-e7eb-4808-a8db-3a7ca17f3922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature {\n",
       "  name: \"gestation_weeks_scaled\"\n",
       "  type: FLOAT\n",
       "  presence {\n",
       "    min_fraction: 1.0\n",
       "  }\n",
       "  shape {\n",
       "  }\n",
       "}\n",
       "feature {\n",
       "  name: \"is_male_index\"\n",
       "  type: INT\n",
       "  int_domain {\n",
       "    min: -1\n",
       "    max: 1\n",
       "    is_categorical: true\n",
       "  }\n",
       "  presence {\n",
       "    min_fraction: 1.0\n",
       "  }\n",
       "  shape {\n",
       "  }\n",
       "}\n",
       "feature {\n",
       "  name: \"is_multiple_index\"\n",
       "  type: INT\n",
       "  int_domain {\n",
       "    min: -1\n",
       "    max: 1\n",
       "    is_categorical: true\n",
       "  }\n",
       "  presence {\n",
       "    min_fraction: 1.0\n",
       "  }\n",
       "  shape {\n",
       "  }\n",
       "}\n",
       "feature {\n",
       "  name: \"mother_age_bucketized\"\n",
       "  type: INT\n",
       "  int_domain {\n",
       "    min: 0\n",
       "    max: 4\n",
       "    is_categorical: true\n",
       "  }\n",
       "  presence {\n",
       "    min_fraction: 1.0\n",
       "  }\n",
       "  shape {\n",
       "  }\n",
       "}\n",
       "feature {\n",
       "  name: \"mother_age_log\"\n",
       "  type: FLOAT\n",
       "  presence {\n",
       "    min_fraction: 1.0\n",
       "  }\n",
       "  shape {\n",
       "  }\n",
       "}\n",
       "feature {\n",
       "  name: \"mother_age_normalized\"\n",
       "  type: FLOAT\n",
       "  presence {\n",
       "    min_fraction: 1.0\n",
       "  }\n",
       "  shape {\n",
       "  }\n",
       "}\n",
       "feature {\n",
       "  name: \"mother_race_index\"\n",
       "  type: INT\n",
       "  int_domain {\n",
       "    min: -1\n",
       "    max: 10\n",
       "    is_categorical: true\n",
       "  }\n",
       "  presence {\n",
       "    min_fraction: 1.0\n",
       "  }\n",
       "  shape {\n",
       "  }\n",
       "}\n",
       "feature {\n",
       "  name: \"weight_pounds\"\n",
       "  type: FLOAT\n",
       "  presence {\n",
       "    min_fraction: 1.0\n",
       "  }\n",
       "  shape {\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_metadata = tft.TFTransformOutput(TRANSFORM_ARTEFACTS_DIR).transformed_metadata\n",
    "transformed_metadata.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1844432b-f5c8-4ba9-9def-ea4e8a198a15",
   "metadata": {},
   "source": [
    "### Define wide and deep feature columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79453a1b-7b9f-4d4a-a117-1e224e490860",
   "metadata": {},
   "source": [
    "This is a feature engineering layer that creates new features from the transformed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0d043c4-bc3f-4b45-9ed9-78e3a2ecc0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_wide_and_deep_feature_columns():\n",
    "\n",
    "    deep_feature_columns = []\n",
    "    wide_feature_columns = []\n",
    "    inputs = {}\n",
    "    categorical_columns = {}\n",
    "\n",
    "    # Select features you've checked from the metadata\n",
    "    # - Categorical features are associated with the vocabulary size (starting from 0)\n",
    "    numeric_features = ['mother_age_log', 'mother_age_normalized', 'gestation_weeks_scaled']\n",
    "    categorical_features = [('is_male_index', 1), ('is_multiple_index', 1),\n",
    "                            ('mother_age_bucketized', 4), ('mother_race_index', 10)]\n",
    "\n",
    "    for feature in numeric_features:\n",
    "        deep_feature_columns.append(tf.feature_column.numeric_column(feature))\n",
    "        inputs[feature] = layers.Input(shape=(), name=feature, dtype='float32')\n",
    "\n",
    "    for feature, vocab_size in categorical_features:\n",
    "        categorical_columns[feature] = (\n",
    "            tf.feature_column.categorical_column_with_identity(feature, num_buckets=vocab_size+1))\n",
    "        wide_feature_columns.append(tf.feature_column.indicator_column(categorical_columns[feature]))\n",
    "        inputs[feature] = layers.Input(shape=(), name=feature, dtype='int64')\n",
    "\n",
    "    mother_race_X_mother_age_bucketized = tf.feature_column.crossed_column(\n",
    "        [categorical_columns['mother_age_bucketized'],\n",
    "         categorical_columns['mother_race_index']],  55)\n",
    "    wide_feature_columns.append(tf.feature_column.indicator_column(mother_race_X_mother_age_bucketized))\n",
    "        \n",
    "    mother_race_X_mother_age_bucketized_embedded = tf.feature_column.embedding_column(\n",
    "        mother_race_X_mother_age_bucketized, 5)        \n",
    "    deep_feature_columns.append(mother_race_X_mother_age_bucketized_embedded)\n",
    "\n",
    "    return wide_feature_columns, deep_feature_columns, inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e117d97c-cb4c-48dd-b8e3-98ff22dbfa4a",
   "metadata": {},
   "source": [
    "### Define a regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64ee68e6-a898-48fc-b27e-c8494d1f9617",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "\n",
    "    wide_feature_columns, deep_feature_columns, inputs = create_wide_and_deep_feature_columns()\n",
    "    feature_layer_wide = layers.DenseFeatures(wide_feature_columns, name='wide_features')\n",
    "    feature_layer_deep = layers.DenseFeatures(deep_feature_columns, name='deep_features')\n",
    "\n",
    "    wide_model = feature_layer_wide(inputs)\n",
    "    \n",
    "    deep_model = layers.Dense(64, activation='relu', name='DNN_layer1')(feature_layer_deep(inputs))\n",
    "    deep_model = layers.Dense(32, activation='relu', name='DNN_layer2')(deep_model)\n",
    "\n",
    "    wide_deep_model = layers.Dense(1, name='weight')(layers.concatenate([wide_model, deep_model]))\n",
    "    model = models.Model(inputs=inputs, outputs=wide_deep_model)\n",
    "\n",
    "    # Compile Keras model\n",
    "    model.compile(loss='mse', optimizer='adam') # tf.keras.optimizers.Adam(learning_rate=0.0001))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536d4d31-69b2-4f68-a016-94f581724fa1",
   "metadata": {},
   "source": [
    "### Define tfrecords_input_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4494e70-37be-4289-b610-467ff1d21e1a",
   "metadata": {},
   "source": [
    "This function creates a batched dataset from the transformed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a7d90fa-fb1b-414a-bc1b-cd3d313de447",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfrecords_input_fn(files_name_pattern, batch_size=512):\n",
    "    \n",
    "    tf_transform_output = tft.TFTransformOutput(TRANSFORM_ARTEFACTS_DIR)\n",
    "    TARGET_FEATURE_NAME = 'weight_pounds'\n",
    "\n",
    "    batched_dataset = tf.data.experimental.make_batched_features_dataset(\n",
    "        file_pattern=files_name_pattern,\n",
    "        batch_size=batch_size,\n",
    "        features=tf_transform_output.transformed_feature_spec(),\n",
    "        reader=tf.data.TFRecordDataset,\n",
    "        label_key=TARGET_FEATURE_NAME,\n",
    "        shuffle=True).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return batched_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4baba0e9-39d3-4e3d-b9be-4401a827b414",
   "metadata": {},
   "source": [
    "## 3. Train and export the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f27fb6f-53ab-44bf-afd0-c55912a866c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(train_pattern, eval_pattern):\n",
    "\n",
    "    train_dataset = tfrecords_input_fn(train_pattern, batch_size=BATCH_SIZE)\n",
    "    validation_dataset = tfrecords_input_fn(eval_pattern, batch_size=BATCH_SIZE)\n",
    "\n",
    "    model = create_model()\n",
    "    print(model.summary())\n",
    "\n",
    "    print('Now training the model... hang on')\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=validation_dataset,\n",
    "        epochs=NUM_EPOCHS,\n",
    "        steps_per_epoch=math.ceil(NUM_TRAIN_INSTANCES / BATCH_SIZE),\n",
    "        validation_steps=math.ceil(NUM_TEST_INSTANCES / BATCH_SIZE),\n",
    "        verbose=0)\n",
    "\n",
    "    print('Evaluate the trained model.')\n",
    "    print(model.evaluate(validation_dataset, steps=NUM_TEST_INSTANCES))\n",
    "    \n",
    "    return history, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21ef6f29-56d5-45dc-942d-fbc0d800bcc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-04 08:04:25.907835: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2022-03-04 08:04:25.907879: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-03-04 08:04:25.907909: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (tensorflow-2-7-20220304-163425): /proc/driver/nvidia/version does not exist\n",
      "2022-03-04 08:04:25.908223: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " gestation_weeks_scaled (InputL  [(None,)]           0           []                               \n",
      " ayer)                                                                                            \n",
      "                                                                                                  \n",
      " is_male_index (InputLayer)     [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " is_multiple_index (InputLayer)  [(None,)]           0           []                               \n",
      "                                                                                                  \n",
      " mother_age_bucketized (InputLa  [(None,)]           0           []                               \n",
      " yer)                                                                                             \n",
      "                                                                                                  \n",
      " mother_age_log (InputLayer)    [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " mother_age_normalized (InputLa  [(None,)]           0           []                               \n",
      " yer)                                                                                             \n",
      "                                                                                                  \n",
      " mother_race_index (InputLayer)  [(None,)]           0           []                               \n",
      "                                                                                                  \n",
      " deep_features (DenseFeatures)  (None, 8)            275         ['gestation_weeks_scaled[0][0]', \n",
      "                                                                  'is_male_index[0][0]',          \n",
      "                                                                  'is_multiple_index[0][0]',      \n",
      "                                                                  'mother_age_bucketized[0][0]',  \n",
      "                                                                  'mother_age_log[0][0]',         \n",
      "                                                                  'mother_age_normalized[0][0]',  \n",
      "                                                                  'mother_race_index[0][0]']      \n",
      "                                                                                                  \n",
      " DNN_layer1 (Dense)             (None, 64)           576         ['deep_features[0][0]']          \n",
      "                                                                                                  \n",
      " wide_features (DenseFeatures)  (None, 75)           0           ['gestation_weeks_scaled[0][0]', \n",
      "                                                                  'is_male_index[0][0]',          \n",
      "                                                                  'is_multiple_index[0][0]',      \n",
      "                                                                  'mother_age_bucketized[0][0]',  \n",
      "                                                                  'mother_age_log[0][0]',         \n",
      "                                                                  'mother_age_normalized[0][0]',  \n",
      "                                                                  'mother_race_index[0][0]']      \n",
      "                                                                                                  \n",
      " DNN_layer2 (Dense)             (None, 32)           2080        ['DNN_layer1[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 107)          0           ['wide_features[0][0]',          \n",
      "                                                                  'DNN_layer2[0][0]']             \n",
      "                                                                                                  \n",
      " weight (Dense)                 (None, 1)            108         ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,039\n",
      "Trainable params: 3,039\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Now training the model... hang on\n",
      "Evaluate the trained model.\n",
      "3000/3000 [==============================] - 70s 23ms/step - loss: 1.1188\n",
      "1.1187626123428345\n"
     ]
    }
   ],
   "source": [
    "train_pattern = os.path.join(TRANSFORMED_DATA_DIR, 'train-*.tfrecords')\n",
    "eval_pattern = os.path.join(TRANSFORMED_DATA_DIR, 'eval-*.tfrecords')\n",
    "\n",
    "DATA_SIZE = 10000\n",
    "BATCH_SIZE = 512\n",
    "NUM_EPOCHS = 100\n",
    "NUM_TRAIN_INSTANCES = math.ceil(DATA_SIZE * 0.7)\n",
    "NUM_TEST_INSTANCES = math.ceil(DATA_SIZE * 0.3)\n",
    "\n",
    "history, trained_model = train_and_evaluate(train_pattern, eval_pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a0d991-5974-4529-aba6-1f4ede30e5a4",
   "metadata": {},
   "source": [
    "### Visualize the training result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a162ca81-bdaa-44c7-9e8c-251d5cb55306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final RMSE for the validation set: 1.053841\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pandas import DataFrame\n",
    "DataFrame({'loss': history.history['loss'], 'val_loss': history.history['val_loss']}).plot(ylim=(0, 2.5))\n",
    "print('Final RMSE for the validation set: {:f}'.format(math.sqrt(history.history['val_loss'][-1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a9522f-5519-4422-9602-4b1e9ffc29aa",
   "metadata": {},
   "source": [
    "### Export the trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe95808-c634-4259-867e-81d5ec52e993",
   "metadata": {},
   "source": [
    "The serving function `serveing_fn` receives raw input data and apply the tranformation before making predictions with the trained model.\n",
    "\n",
    "You can also add some pre/post-processing within the seriving function. In this example, it accepts an unique identifier for each instance and passes through it to the output, that is useful for batch predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f69ec54f-80f1-457a-8dd4-f00004450cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_serving_model(model, output_dir):\n",
    "\n",
    "    tf_transform_output = tft.TFTransformOutput(TRANSFORM_ARTEFACTS_DIR)\n",
    "    # The layer has to be saved to the model for keras tracking purposes.\n",
    "    model.tft_layer = tf_transform_output.transform_features_layer()\n",
    "\n",
    "    @tf.function\n",
    "    def serveing_fn(uid, is_male, mother_race, mother_age, plurality, gestation_weeks):\n",
    "        features = {\n",
    "            'is_male': is_male, \n",
    "            'mother_race': mother_race, \n",
    "            'mother_age': mother_age, \n",
    "            'plurality': plurality, \n",
    "            'gestation_weeks': gestation_weeks\n",
    "        } \n",
    "        transformed_features = model.tft_layer(features)\n",
    "        outputs = model(transformed_features)\n",
    "        # The prediction results have multiple elements in general.\n",
    "        # But we need only the first element in our case.\n",
    "        outputs = tf.map_fn(lambda item: item[0], outputs)\n",
    "\n",
    "        return {'uid': uid, 'weight': outputs}\n",
    "\n",
    "    concrete_serving_fn = serveing_fn.get_concrete_function(\n",
    "        tf.TensorSpec(shape=[None], dtype=tf.string, name='uid'),\n",
    "        tf.TensorSpec(shape=[None], dtype=tf.string, name='is_male'),\n",
    "        tf.TensorSpec(shape=[None], dtype=tf.string, name='mother_race'),\n",
    "        tf.TensorSpec(shape=[None], dtype=tf.float32, name='mother_age'),\n",
    "        tf.TensorSpec(shape=[None], dtype=tf.float32, name='plurality'),\n",
    "        tf.TensorSpec(shape=[None], dtype=tf.float32, name='gestation_weeks')\n",
    "    )\n",
    "    signatures = {'serving_default': concrete_serving_fn}\n",
    "\n",
    "    model.save(output_dir, save_format='tf', signatures=signatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70cbc8dc-53fb-4ee4-9cf2-ea889437e8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_text is not available.\n",
      "INFO:tensorflow:tensorflow_decision_forests is not available.\n",
      "INFO:tensorflow:struct2tensor is not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-04 08:08:28.376970: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: gs://your-project-babyweight/babyweight_tft/models/export/assets\n"
     ]
    }
   ],
   "source": [
    "EXPORT_DIR = os.path.join(MODELS_DIR, 'export')\n",
    "export_serving_model(trained_model, EXPORT_DIR)\n",
    "os.environ['EXPORT_DIR'] = EXPORT_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73264505-d99d-4ec4-90e5-5d01f2422dd9",
   "metadata": {},
   "source": [
    "### Explore the exported model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52cb23d9-8cf2-42ab-8187-5dff62b1a449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://your-project-babyweight/babyweight_tft/models/export/:\n",
      "         0  2022-03-04T08:08:31Z  gs://your-project-babyweight/babyweight_tft/models/export/\n",
      "     26766  2022-03-04T08:08:38Z  gs://your-project-babyweight/babyweight_tft/models/export/keras_metadata.pb\n",
      "    723728  2022-03-04T08:08:38Z  gs://your-project-babyweight/babyweight_tft/models/export/saved_model.pb\n",
      "\n",
      "gs://your-project-babyweight/babyweight_tft/models/export/assets/:\n",
      "         0  2022-03-04T08:08:35Z  gs://your-project-babyweight/babyweight_tft/models/export/assets/\n",
      "        11  2022-03-04T08:08:36Z  gs://your-project-babyweight/babyweight_tft/models/export/assets/is_male\n",
      "        11  2022-03-04T08:08:37Z  gs://your-project-babyweight/babyweight_tft/models/export/assets/is_multiple\n",
      "       102  2022-03-04T08:08:35Z  gs://your-project-babyweight/babyweight_tft/models/export/assets/mother_race\n",
      "\n",
      "gs://your-project-babyweight/babyweight_tft/models/export/variables/:\n",
      "         0  2022-03-04T08:08:31Z  gs://your-project-babyweight/babyweight_tft/models/export/variables/\n",
      "     43889  2022-03-04T08:08:33Z  gs://your-project-babyweight/babyweight_tft/models/export/variables/variables.data-00000-of-00001\n",
      "      1949  2022-03-04T08:08:34Z  gs://your-project-babyweight/babyweight_tft/models/export/variables/variables.index\n",
      "TOTAL: 10 objects, 796456 bytes (777.79 KiB)\n",
      "\n",
      "MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n",
      "\n",
      "signature_def['__saved_model_init_op']:\n",
      "  The given SavedModel SignatureDef contains the following input(s):\n",
      "  The given SavedModel SignatureDef contains the following output(s):\n",
      "    outputs['__saved_model_init_op'] tensor_info:\n",
      "        dtype: DT_INVALID\n",
      "        shape: unknown_rank\n",
      "        name: NoOp\n",
      "  Method name is: \n",
      "\n",
      "signature_def['serving_default']:\n",
      "  The given SavedModel SignatureDef contains the following input(s):\n",
      "    inputs['gestation_weeks'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1)\n",
      "        name: serving_default_gestation_weeks:0\n",
      "    inputs['is_male'] tensor_info:\n",
      "        dtype: DT_STRING\n",
      "        shape: (-1)\n",
      "        name: serving_default_is_male:0\n",
      "    inputs['mother_age'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1)\n",
      "        name: serving_default_mother_age:0\n",
      "    inputs['mother_race'] tensor_info:\n",
      "        dtype: DT_STRING\n",
      "        shape: (-1)\n",
      "        name: serving_default_mother_race:0\n",
      "    inputs['plurality'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1)\n",
      "        name: serving_default_plurality:0\n",
      "    inputs['uid'] tensor_info:\n",
      "        dtype: DT_STRING\n",
      "        shape: (-1)\n",
      "        name: serving_default_uid:0\n",
      "  The given SavedModel SignatureDef contains the following output(s):\n",
      "    outputs['uid'] tensor_info:\n",
      "        dtype: DT_STRING\n",
      "        shape: (-1)\n",
      "        name: StatefulPartitionedCall_6:0\n",
      "    outputs['weight'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1)\n",
      "        name: StatefulPartitionedCall_6:1\n",
      "  Method name is: tensorflow/serving/predict\n",
      "\n",
      "Defined Functions:\n",
      "  Function Name: '__call__'\n",
      "    Option #1\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          DType: dict\n",
      "          Value: {'is_male_index': TensorSpec(shape=(None,), dtype=tf.int64, name='inputs/is_male_index'), 'gestation_weeks_scaled': TensorSpec(shape=(None,), dtype=tf.float32, name='inputs/gestation_weeks_scaled'), 'mother_age_normalized': TensorSpec(shape=(None,), dtype=tf.float32, name='inputs/mother_age_normalized'), 'is_multiple_index': TensorSpec(shape=(None,), dtype=tf.int64, name='inputs/is_multiple_index'), 'mother_age_bucketized': TensorSpec(shape=(None,), dtype=tf.int64, name='inputs/mother_age_bucketized'), 'mother_race_index': TensorSpec(shape=(None,), dtype=tf.int64, name='inputs/mother_race_index'), 'mother_age_log': TensorSpec(shape=(None,), dtype=tf.float32, name='inputs/mother_age_log')}\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: False\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "    Option #2\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          DType: dict\n",
      "          Value: {'gestation_weeks_scaled': TensorSpec(shape=(None,), dtype=tf.float32, name='gestation_weeks_scaled'), 'is_multiple_index': TensorSpec(shape=(None,), dtype=tf.int64, name='is_multiple_index'), 'mother_age_bucketized': TensorSpec(shape=(None,), dtype=tf.int64, name='mother_age_bucketized'), 'mother_race_index': TensorSpec(shape=(None,), dtype=tf.int64, name='mother_race_index'), 'mother_age_normalized': TensorSpec(shape=(None,), dtype=tf.float32, name='mother_age_normalized'), 'mother_age_log': TensorSpec(shape=(None,), dtype=tf.float32, name='mother_age_log'), 'is_male_index': TensorSpec(shape=(None,), dtype=tf.int64, name='is_male_index')}\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: False\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "    Option #3\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          DType: dict\n",
      "          Value: {'is_multiple_index': TensorSpec(shape=(None,), dtype=tf.int64, name='is_multiple_index'), 'gestation_weeks_scaled': TensorSpec(shape=(None,), dtype=tf.float32, name='gestation_weeks_scaled'), 'mother_race_index': TensorSpec(shape=(None,), dtype=tf.int64, name='mother_race_index'), 'is_male_index': TensorSpec(shape=(None,), dtype=tf.int64, name='is_male_index'), 'mother_age_bucketized': TensorSpec(shape=(None,), dtype=tf.int64, name='mother_age_bucketized'), 'mother_age_log': TensorSpec(shape=(None,), dtype=tf.float32, name='mother_age_log'), 'mother_age_normalized': TensorSpec(shape=(None,), dtype=tf.float32, name='mother_age_normalized')}\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: True\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "    Option #4\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          DType: dict\n",
      "          Value: {'mother_race_index': TensorSpec(shape=(None,), dtype=tf.int64, name='inputs/mother_race_index'), 'mother_age_bucketized': TensorSpec(shape=(None,), dtype=tf.int64, name='inputs/mother_age_bucketized'), 'is_male_index': TensorSpec(shape=(None,), dtype=tf.int64, name='inputs/is_male_index'), 'is_multiple_index': TensorSpec(shape=(None,), dtype=tf.int64, name='inputs/is_multiple_index'), 'mother_age_normalized': TensorSpec(shape=(None,), dtype=tf.float32, name='inputs/mother_age_normalized'), 'mother_age_log': TensorSpec(shape=(None,), dtype=tf.float32, name='inputs/mother_age_log'), 'gestation_weeks_scaled': TensorSpec(shape=(None,), dtype=tf.float32, name='inputs/gestation_weeks_scaled')}\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: True\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "\n",
      "  Function Name: '_default_save_signature'\n",
      "    Option #1\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          DType: dict\n",
      "          Value: {'gestation_weeks_scaled': TensorSpec(shape=(None,), dtype=tf.float32, name='gestation_weeks_scaled'), 'is_male_index': TensorSpec(shape=(None,), dtype=tf.int64, name='is_male_index'), 'is_multiple_index': TensorSpec(shape=(None,), dtype=tf.int64, name='is_multiple_index'), 'mother_race_index': TensorSpec(shape=(None,), dtype=tf.int64, name='mother_race_index'), 'mother_age_bucketized': TensorSpec(shape=(None,), dtype=tf.int64, name='mother_age_bucketized'), 'mother_age_normalized': TensorSpec(shape=(None,), dtype=tf.float32, name='mother_age_normalized'), 'mother_age_log': TensorSpec(shape=(None,), dtype=tf.float32, name='mother_age_log')}\n",
      "\n",
      "  Function Name: 'call_and_return_all_conditional_losses'\n",
      "    Option #1\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          DType: dict\n",
      "          Value: {'mother_race_index': TensorSpec(shape=(None,), dtype=tf.int64, name='inputs/mother_race_index'), 'is_multiple_index': TensorSpec(shape=(None,), dtype=tf.int64, name='inputs/is_multiple_index'), 'gestation_weeks_scaled': TensorSpec(shape=(None,), dtype=tf.float32, name='inputs/gestation_weeks_scaled'), 'mother_age_bucketized': TensorSpec(shape=(None,), dtype=tf.int64, name='inputs/mother_age_bucketized'), 'mother_age_normalized': TensorSpec(shape=(None,), dtype=tf.float32, name='inputs/mother_age_normalized'), 'mother_age_log': TensorSpec(shape=(None,), dtype=tf.float32, name='inputs/mother_age_log'), 'is_male_index': TensorSpec(shape=(None,), dtype=tf.int64, name='inputs/is_male_index')}\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: False\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "    Option #2\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          DType: dict\n",
      "          Value: {'mother_age_log': TensorSpec(shape=(None,), dtype=tf.float32, name='mother_age_log'), 'mother_age_bucketized': TensorSpec(shape=(None,), dtype=tf.int64, name='mother_age_bucketized'), 'mother_age_normalized': TensorSpec(shape=(None,), dtype=tf.float32, name='mother_age_normalized'), 'is_male_index': TensorSpec(shape=(None,), dtype=tf.int64, name='is_male_index'), 'gestation_weeks_scaled': TensorSpec(shape=(None,), dtype=tf.float32, name='gestation_weeks_scaled'), 'mother_race_index': TensorSpec(shape=(None,), dtype=tf.int64, name='mother_race_index'), 'is_multiple_index': TensorSpec(shape=(None,), dtype=tf.int64, name='is_multiple_index')}\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: False\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "    Option #3\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          DType: dict\n",
      "          Value: {'mother_age_normalized': TensorSpec(shape=(None,), dtype=tf.float32, name='mother_age_normalized'), 'is_multiple_index': TensorSpec(shape=(None,), dtype=tf.int64, name='is_multiple_index'), 'mother_race_index': TensorSpec(shape=(None,), dtype=tf.int64, name='mother_race_index'), 'mother_age_log': TensorSpec(shape=(None,), dtype=tf.float32, name='mother_age_log'), 'is_male_index': TensorSpec(shape=(None,), dtype=tf.int64, name='is_male_index'), 'mother_age_bucketized': TensorSpec(shape=(None,), dtype=tf.int64, name='mother_age_bucketized'), 'gestation_weeks_scaled': TensorSpec(shape=(None,), dtype=tf.float32, name='gestation_weeks_scaled')}\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: True\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "    Option #4\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          DType: dict\n",
      "          Value: {'mother_age_normalized': TensorSpec(shape=(None,), dtype=tf.float32, name='inputs/mother_age_normalized'), 'is_male_index': TensorSpec(shape=(None,), dtype=tf.int64, name='inputs/is_male_index'), 'mother_race_index': TensorSpec(shape=(None,), dtype=tf.int64, name='inputs/mother_race_index'), 'is_multiple_index': TensorSpec(shape=(None,), dtype=tf.int64, name='inputs/is_multiple_index'), 'gestation_weeks_scaled': TensorSpec(shape=(None,), dtype=tf.float32, name='inputs/gestation_weeks_scaled'), 'mother_age_bucketized': TensorSpec(shape=(None,), dtype=tf.int64, name='inputs/mother_age_bucketized'), 'mother_age_log': TensorSpec(shape=(None,), dtype=tf.float32, name='inputs/mother_age_log')}\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: True\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-04 08:08:45.817672: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2022-03-04 08:08:45.817721: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-03-04 08:08:45.817744: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (tensorflow-2-7-20220304-163425): /proc/driver/nvidia/version does not exist\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gsutil ls -lR $EXPORT_DIR\n",
    "saved_model_cli show --all --dir=$EXPORT_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f98b507-2101-406e-9f23-d25865341a85",
   "metadata": {},
   "source": [
    "## 4. Use the exported model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a642c580-fd0d-43ac-85e7-241505ca2203",
   "metadata": {},
   "source": [
    "### Load the exported model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d6661d7-a9be-459e-8a9f-08f2a7aa0669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x7fefe4dd0ed0> and <keras.engine.input_layer.InputLayer object at 0x7fefe4ece390>).\n"
     ]
    }
   ],
   "source": [
    "#tf.keras.backend.clear_session()\n",
    "model = tf.keras.models.load_model(EXPORT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4fa8ee-7ffd-4013-805a-01b7b83d3e31",
   "metadata": {},
   "source": [
    "### Define a local prediction function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432b4d1c-00b5-4d76-970b-08aab452711e",
   "metadata": {},
   "source": [
    "The serving function of the exported model accepts input data with named options. You can use this wrapper to use input data in the standard python dictionary.\n",
    "\n",
    "When you deply the model to Vertex AI using the pre-built container, you can use the Python client library to make online predictions without the wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e23303f7-520a-4bca-ae30-42e9a09c46f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(requests):\n",
    "\n",
    "    uid, is_male, mother_race, mother_age, plurality, gestation_weeks = [], [], [], [], [], []\n",
    "    for instance in requests:\n",
    "        uid.append(instance['uid'])\n",
    "        is_male.append(instance['is_male'])\n",
    "        mother_race.append(instance['mother_race'])\n",
    "        mother_age.append(instance['mother_age'])\n",
    "        plurality.append(instance['plurality'])\n",
    "        gestation_weeks.append(float(instance['gestation_weeks']))\n",
    "\n",
    "    result = model.signatures['serving_default'](\n",
    "        uid = tf.convert_to_tensor(uid),\n",
    "        is_male = tf.convert_to_tensor(is_male),\n",
    "        mother_race = tf.convert_to_tensor(mother_race),\n",
    "        mother_age = tf.convert_to_tensor(mother_age),\n",
    "        plurality = tf.convert_to_tensor(plurality),\n",
    "        gestation_weeks = tf.convert_to_tensor(gestation_weeks))\n",
    "\n",
    "    result = zip(result['uid'].numpy().tolist(), result['weight'].numpy().tolist())\n",
    "    result = [{'uid': output[0].decode('ascii'), 'weight': output[1]} for output in result]\n",
    "    return {'predictions': result}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada1864e-3836-4d09-93c4-7a64c48a7184",
   "metadata": {},
   "source": [
    "### Make local predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1b4fc11-c3ee-4e75-8aea-0b145dbfbeb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'predictions': [{'uid': 'instance1', 'weight': 6.888402938842773},\n",
       "  {'uid': 'instance2', 'weight': 5.197094440460205}]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance1 = {\n",
    "    'uid': 'instance1',\n",
    "    'is_male': 'True',\n",
    "    'mother_age': 26.0,\n",
    "    'mother_race': 'Asian Indian',\n",
    "    'plurality': 1.0,\n",
    "    'gestation_weeks': 39\n",
    "}\n",
    "\n",
    "instance2 = {\n",
    "    'uid': 'instance2',\n",
    "    'is_male': 'False',\n",
    "    'mother_age': 40.0,\n",
    "    'mother_race': 'Japanese',\n",
    "    'plurality': 2.0,\n",
    "    'gestation_weeks': 37\n",
    "}\n",
    "\n",
    "predict([instance1, instance2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7aad480-f3b4-41a8-9d0f-59ec945cec3f",
   "metadata": {},
   "source": [
    "### Deploy model to Vertex AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503d3cfd-1583-494a-8679-6467174868f9",
   "metadata": {},
   "source": [
    "Deployment of the model may take minutes. Please hang on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fdcbfd-7fe1-4fb9-a8a3-6dc4057a56ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "ts=$(date +%y%m%d-%H%M%S)\n",
    "MODEL_NAME=\"babyweight-$ts\"\n",
    "ENDPOINT_NAME=\"babyweight-endpoint-$ts\"\n",
    "\n",
    "gcloud ai models upload --region=$REGION --display-name=$MODEL_NAME \\\n",
    "  --container-image-uri=us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-7:latest \\\n",
    "  --artifact-uri=$EXPORT_DIR\n",
    "MODEL_ID=$(gcloud ai models list --region=$REGION \\\n",
    "  --filter=display_name=$MODEL_NAME --format \"value(MODEL_ID)\")\n",
    "\n",
    "gcloud ai endpoints create --region=$REGION --display-name=$ENDPOINT_NAME\n",
    "ENDPOINT_ID=$(gcloud ai endpoints list --region=$REGION \\\n",
    "  --filter=display_name=$ENDPOINT_NAME --format \"value(ENDPOINT_ID)\")\n",
    "\n",
    "gcloud ai endpoints deploy-model $ENDPOINT_ID \\\n",
    "  --region=$REGION --model=$MODEL_ID --display-name=$MODEL_NAME \\\n",
    "  --machine-type=n1-standard-2 \\\n",
    "  --min-replica-count=1 --max-replica-count=3 --traffic-split=0=100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c331827c-63dd-4b46-9f4c-0775c3a00f21",
   "metadata": {},
   "source": [
    "### Make online preditions with Python client library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7a5ddc-9b69-4c26-870c-772156cf23d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "# Use the latest endpoint that starts with 'babyweight-endpoint'\n",
    "endpoints = aiplatform.Endpoint.list(\n",
    "    project=PROJECT, location=REGION, order_by='create_time desc')\n",
    "for item in endpoints:\n",
    "    if item.display_name.startswith('babyweight-endpoint'):\n",
    "        endpoint = item\n",
    "        break\n",
    "\n",
    "endpoint.predict(instances=[instance1, instance2]).predictions"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-7.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-7:m87"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
